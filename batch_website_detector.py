#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
äº’è”ç½‘è¿æ³•ç½‘ç«™æ‰¹é‡æ£€æµ‹è„šæœ¬
åŸºäºå¤šç»´åº¦ç‰¹å¾åˆ†æçš„è‡ªåŠ¨åŒ–æ£€æµ‹ç³»ç»Ÿ

åŠŸèƒ½ç‰¹ç‚¹ï¼š
1. æ‰¹é‡URLæ£€æµ‹
2. å¤šç»´åº¦ç‰¹å¾æå–
3. æœºå™¨å­¦ä¹ æ¨¡å‹é›†æˆ
4. å®æ—¶ç»“æœè¾“å‡º
5. è¯¦ç»†æŠ¥å‘Šç”Ÿæˆ
"""

import requests
import re
import socket
import ssl
import whois
import datetime
import json
import csv
import os
import time
import hashlib
from urllib.parse import urlparse, urljoin
from bs4 import BeautifulSoup
import dns.resolver
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
import joblib
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
import warnings
import pymysql 
import signal
warnings.filterwarnings('ignore')
# è¯»å–é…ç½®æ–‡ä»¶
def load_config(config_path='config.json'):
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    default_config = {
        'db_config': {
            'host': '192.168.2.41',  # æ•°æ®åº“ä¸»æœºåœ°å€
            'port': 3306,
            'user': 'root',       # æ•°æ®åº“ç”¨æˆ·å
            'password': 'df!2020?OK',  # æ•°æ®åº“å¯†ç 
            'db': 'ntmv3',  # æ•°æ®åº“åç§°
            'charset': 'utf8mb4'
        },
        'max_workers': 10,
        'timeout': 10,
        'max_subpages': 50,
        'cache_ttl': 3600
    }
    
    if os.path.exists(config_path):
        with open(config_path, 'r', encoding='utf-8') as f:
            user_config = json.load(f)
            default_config.update(user_config)
    # å¤„ç†use_dict_cursoré…ç½®
    if 'db_config' in default_config and default_config['db_config'].pop('use_dict_cursor', False):
        default_config['db_config']['cursorclass'] = pymysql.cursors.DictCursor
    return default_config
# æ•°æ®åº“é…ç½®
CONFIG = load_config()
DB_CONFIG = CONFIG['db_config']
# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# IPåœ°å€æ­£åˆ™è¡¨è¾¾å¼
IP_PATTERN = re.compile(r'^(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)$')

# åŸŸåæ­£åˆ™è¡¨è¾¾å¼ï¼ˆç®€åŒ–ç‰ˆï¼‰
DOMAIN_PATTERN = re.compile(r'^[a-zA-Z0-9][a-zA-Z0-9-]{0,61}[a-zA-Z0-9](?:\.[a-zA-Z]{2,})+$')


def is_ip_address(text):
    """åˆ¤æ–­æ–‡æœ¬æ˜¯å¦ä¸ºIPåœ°å€"""
    return bool(IP_PATTERN.match(text))

def is_domain(text):
    """åˆ¤æ–­æ–‡æœ¬æ˜¯å¦ä¸ºåŸŸå"""
    return bool(DOMAIN_PATTERN.match(text))

def extract_domain_from_url(url):
    """ä»URLä¸­æå–åŸŸå"""
    # å¦‚æœURLä»¥http://æˆ–https://å¼€å¤´ï¼Œå»æ‰åè®®éƒ¨åˆ†
    if url.startswith('http://'):
        url = url[7:]
    elif url.startswith('https://'):
        url = url[8:]
    
    # å»æ‰è·¯å¾„éƒ¨åˆ†ï¼Œåªä¿ç•™åŸŸå
    domain = url.split('/')[0].split('?')[0].split(':')[0]
    return domain

def update_blacklist_from_db():
    """ä»æ•°æ®åº“æ›´æ–°é»‘åå•æ–‡ä»¶"""
    try:
        # è¿æ¥æ•°æ®åº“
        connection = pymysql.connect(**DB_CONFIG)
        
        # åˆ›å»ºæ¸¸æ ‡
        with connection.cursor() as cursor:
            # æ‰§è¡ŒSQLæŸ¥è¯¢
            sql = "SELECT site_url, rel_url FROM gat_violat_chap;"
            cursor.execute(sql)
            results = cursor.fetchall()
            
            # æå–æ‰€æœ‰URL
            all_urls = []
            for row in results:
                if row['site_url']:
                    all_urls.append(row['site_url'])
                if row['rel_url']:
                    all_urls.append(row['rel_url'])
            
            # å»é‡
            unique_urls = list(set(all_urls))
            
            # åˆ†åˆ«å­˜å‚¨IPå’ŒåŸŸå
            ips = set()
            domains = set()
            
            for url in unique_urls:
                # æå–åŸŸåæˆ–IP
                if is_ip_address(url):
                    ips.add(url)
                elif is_domain(url):
                    domains.add(url)
                else:
                    # å°è¯•ä»URLä¸­æå–åŸŸå
                    extracted = extract_domain_from_url(url)
                    if is_ip_address(extracted):
                        ips.add(extracted)
                    elif is_domain(extracted):
                        domains.add(extracted)
            
            # è·å–å½“å‰ç›®å½•
            current_dir = os.path.dirname(os.path.abspath(__file__))
            
            # å†™å…¥blacklist_ips.txt
            ip_file = os.path.join(current_dir, 'blacklist_ips.txt')
            with open(ip_file, 'w', encoding='utf-8') as f:
                for ip in sorted(ips):
                    f.write(f"{ip}\n")
            
            # å†™å…¥blacklist_domains.txt
            domain_file = os.path.join(current_dir, 'blacklist_domains.txt')
            with open(domain_file, 'w', encoding='utf-8') as f:
                for domain in sorted(domains):
                    f.write(f"{domain}\n")
            
            print(f"âœ… æˆåŠŸæ›´æ–°é»‘åå•æ–‡ä»¶ï¼")
            print(f"   - æ–°å¢ {len(ips)} ä¸ªIPåœ°å€åˆ° {ip_file}")
            print(f"   - æ–°å¢ {len(domains)} ä¸ªåŸŸååˆ° {domain_file}")
            
    except Exception as e:
        print(f"âŒ ä»æ•°æ®åº“æ›´æ–°é»‘åå•å¤±è´¥: {e}")
    finally:
        if 'connection' in locals() and connection.open:
            connection.close()
# å½©è‰²è¾“å‡ºå·¥å…·ç±»
class ColorPrinter:
    """å½©è‰²è¾“å‡ºå·¥å…·ç±»"""
    
    COLORS = {
        'red': '\033[91m',
        'green': '\033[92m',
        'yellow': '\033[93m',
        'blue': '\033[94m',
        'magenta': '\033[95m',
        'cyan': '\033[96m',
        'white': '\033[97m',
        'reset': '\033[0m'
    }
    
    @classmethod
    def print_header(cls, text):
        """æ‰“å°æ ‡é¢˜"""
        print(f"{cls.COLORS['cyan']}{text}{cls.COLORS['reset']}")
    
    @classmethod
    def print_info(cls, text):
        """æ‰“å°ä¿¡æ¯"""
        print(f"{cls.COLORS['blue']}â„¹ï¸  {text}{cls.COLORS['reset']}")
    
    @classmethod
    def print_success(cls, text):
        """æ‰“å°æˆåŠŸä¿¡æ¯"""
        print(f"{cls.COLORS['green']}âœ… {text}{cls.COLORS['reset']}")
    
    @classmethod
    def print_warning(cls, text):
        """æ‰“å°è­¦å‘Šä¿¡æ¯"""
        print(f"{cls.COLORS['yellow']}âš ï¸  {text}{cls.COLORS['reset']}")
    
    @classmethod
    def print_error(cls, text):
        """æ‰“å°é”™è¯¯ä¿¡æ¯"""
        print(f"{cls.COLORS['red']}âŒ {text}{cls.COLORS['reset']}")
    
    @classmethod
    def print_risk(cls, text, level):
        """æ ¹æ®é£é™©ç­‰çº§æ‰“å°å½©è‰²ä¿¡æ¯"""
        if level == 'é«˜':
            print(f"{cls.COLORS['red']}ğŸ”´ {text}{cls.COLORS['reset']}")
        elif level == 'ä¸­':
            print(f"{cls.COLORS['yellow']}ğŸŸ¡ {text}{cls.COLORS['reset']}")
        elif level == 'ä½':
            print(f"{cls.COLORS['green']}ğŸŸ¢ {text}{cls.COLORS['reset']}")
    
    @classmethod
    def print_progress(cls, text):
        """æ‰“å°è¿›åº¦ä¿¡æ¯"""
        print(f"{cls.COLORS['magenta']}â³ {text}{cls.COLORS['reset']}")
    
    def __init__(self):
        self.enabled = True
        try:
            import ctypes
            kernel32 = ctypes.windll.kernel32
            kernel32.SetConsoleMode(kernel32.GetStdHandle(-11), 7)
        except:
            self.enabled = False
    
    def print(self, text, color='white', bold=False, end='\n'):
        """å½©è‰²æ‰“å°"""
        if not self.enabled:
            print(text, end=end)
            return
            
        colors = {
            'red': '\033[91m',
            'green': '\033[92m', 
            'yellow': '\033[93m',
            'blue': '\033[94m',
            'magenta': '\033[95m',
            'cyan': '\033[96m',
            'white': '\033[97m',
            'reset': '\033[0m'
        }
        
        color_code = colors.get(color, colors['white'])
        bold_code = '\033[1m' if bold else ''
        reset_code = colors['reset']
        
        print(f"{bold_code}{color_code}{text}{reset_code}", end=end)
    
    def print_header(self, text):
        """æ‰“å°æ ‡é¢˜"""
        self.print("=" * 60, 'cyan', bold=True)
        self.print(text.center(60), 'cyan', bold=True)
        self.print("=" * 60, 'cyan', bold=True)
    
    def print_risk_level(self, risk_level, url, score):
        """æ‰“å°é£é™©ç­‰çº§"""
        risk_colors = {
            'é«˜é£é™©': 'red',
            'ä¸­é£é™©': 'yellow', 
            'ä½é£é™©': 'green',
            'æ£€æµ‹å¤±è´¥': 'magenta'
        }
        
        color = risk_colors.get(risk_level, 'white')
        emoji = {
            'é«˜é£é™©': 'ğŸš¨',
            'ä¸­é£é™©': 'âš ï¸',
            'ä½é£é™©': 'âœ…',
            'æ£€æµ‹å¤±è´¥': 'âŒ'
        }.get(risk_level, 'â€¢')
        
        self.print(f"{emoji} {url}", color, bold=True)
        self.print(f"   é£é™©ç­‰çº§: {risk_level} ({score}%)", color)
    
    def print_progress(self, current, total, url, risk_level):
        """æ‰“å°è¿›åº¦æ¡"""
        progress = current / total
        bar_length = 30
        filled = int(bar_length * progress)
        bar = 'â–ˆ' * filled + 'â–‘' * (bar_length - filled)
        
        colors = {
            'é«˜é£é™©': 'red',
            'ä¸­é£é™©': 'yellow',
            'ä½é£é™©': 'green',
            'æ£€æµ‹å¤±è´¥': 'magenta'
        }
        color = colors.get(risk_level, 'white')
        
        self.print(f"\r[{bar}] {current}/{total} - {url[:50]}...", color, end='')
    
    def print_summary(self, results):
        """æ‰“å°å½©è‰²ç»Ÿè®¡æ‘˜è¦"""
        if not results:
            return
            
        total = len(results)
        risk_counts = {}
        for result in results:
            risk_level = result.get('é£é™©ç­‰çº§', 'æœªçŸ¥')
            risk_counts[risk_level] = risk_counts.get(risk_level, 0) + 1
        
        self.print("\n" + "=" * 60, 'cyan', bold=True)
        self.print("ğŸ“Š æ£€æµ‹ç»Ÿè®¡æ±‡æ€»".center(60), 'cyan', bold=True)
        self.print("=" * 60, 'cyan', bold=True)
        
        risk_styles = {
            'é«˜é£é™©': ('red', 'ğŸš¨'),
            'ä¸­é£é™©': ('yellow', 'âš ï¸'),
            'ä½é£é™©': ('green', 'âœ…'),
            'æ£€æµ‹å¤±è´¥': ('magenta', 'âŒ')
        }
        
        for risk_level, count in sorted(risk_counts.items()):
            color, emoji = risk_styles.get(risk_level, ('white', 'â€¢'))
            percentage = (count / total) * 100
            self.print(f"{emoji} {risk_level}: {count} ä¸ª ({percentage:.1f}%)", color)
        
        self.print("-" * 60, 'cyan')
        
        # å®‰å…¨å»ºè®®
        high_risk = risk_counts.get('é«˜é£é™©', 0)
        medium_risk = risk_counts.get('ä¸­é£é™©', 0)
        
        if high_risk > 0:
            self.print("ğŸš¨ ç«‹å³å¤„ç†: å‘ç°é«˜é£é™©ç½‘ç«™ï¼Œè¯·ç«‹å³å¤„ç†ï¼", 'red', bold=True)
        if medium_risk > 0:
            self.print("âš ï¸ è°¨æ…è®¿é—®: å‘ç°ä¸­é£é™©ç½‘ç«™ï¼Œå»ºè®®è¿›ä¸€æ­¥éªŒè¯", 'yellow', bold=True)
        if high_risk == 0 and medium_risk == 0:
            self.print("âœ… å®‰å…¨è‰¯å¥½: æœ¬æ¬¡æ£€æµ‹æœªå‘ç°æ˜æ˜¾é£é™©ç½‘ç«™", 'green', bold=True)

# åˆ›å»ºå…¨å±€å½©è‰²æ‰“å°å®ä¾‹
color_printer = ColorPrinter()

# ä¿®æ”¹åŸæœ‰çš„æ‰“å°å‡½æ•°ä½¿ç”¨å½©è‰²è¾“å‡º
def print_colored_detection_result(result):
    """å½©è‰²æ‰“å°æ£€æµ‹ç»“æœ"""
    url = result.get('ç½‘å€', 'æœªçŸ¥')
    risk_level = result.get('é£é™©ç­‰çº§', 'æœªçŸ¥')
    risk_score = result.get('é£é™©è¯„åˆ†', '0%')
    risk_desc = result.get('é£é™©æè¿°', '')
    
    color_printer.print_risk_level(risk_level, url, risk_score)
    
    if risk_desc:
        lines = risk_desc.split('\n')
        for line in lines:
            if line.startswith('ğŸš¨') or line.startswith('âš ï¸'):
                color_printer.print(f"   {line}", 'red' if 'ğŸš¨' in line else 'yellow')
            elif line.startswith('âœ…'):
                color_printer.print(f"   {line}", 'green')
            elif line.startswith('ğŸ’¡'):
                color_printer.print(f"   {line}", 'blue')
            elif line.startswith('â€¢'):
                color_printer.print(f"   {line}", 'white')
    
    print()  # ç©ºè¡Œåˆ†éš”

class WebsiteDetector:
    """è¿æ³•ç½‘ç«™æ£€æµ‹å™¨ç±»"""
    # æ·»åŠ ç±»çº§ç¼“å­˜
    _keyword_cache = None
    _cache_timestamp = 0
    _cache_ttl = 3600  # ç¼“å­˜æœ‰æ•ˆæœŸï¼Œå•ä½ç§’
    
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        self.timeout = 10
        # æ·»åŠ å­é¡µé¢æ£€æµ‹ç›¸å…³å‚æ•°
        self.max_subpages = 50  # æœ€å¤šæ£€æµ‹çš„å­é¡µé¢æ•°é‡
        self.subpage_timeout = 8  # å­é¡µé¢æ£€æµ‹è¶…æ—¶æ—¶é—´
        
        # æ•æ„Ÿå…³é”®è¯åº“ - æ‰©å±•åˆ†ç±»
        self.sensitive_keywords = self._load_keywords_from_db()
        
        # å¯ç–‘åŸŸååç¼€ - æ‰©å±•åˆ—è¡¨
        self.suspicious_tlds = [
            '.tk', '.ml', '.ga', '.cf', '.gq', '.top', '.wang', '.ren', 
            '.click', '.download', '.link', '.work', '.party', '.racing',
            '.date', '.accountant', '.science', '.trade', '.review'
        ]
        
        # çŸ¥åå“ç‰Œçš„é’“é±¼åŸŸåå˜ä½“æ£€æµ‹
        self.brand_keywords = [
            'alipay', 'taobao', 'tmall', 'jd', 'qq', 'wechat', 'bank',
            'icbc', 'ccb', 'abc', 'boc', 'unionpay', 'paypal', 'amazon',
            'microsoft', 'google', 'apple', 'facebook', 'instagram'
        ]
        
        # é»‘åå•IPæ®µå’ŒåŸŸå
        self.blacklisted_ips = set()
        self.blacklisted_domains = set()
        self._load_blacklists()
        
        # åŠ è½½æ¨¡å‹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        self.model = self._load_model()
        
        # å¯ä¿¡CAåˆ—è¡¨
        self.trusted_cas = [
            "Let's Encrypt", "DigiCert", "GlobalSign", "Sectigo", "GoDaddy",
            "Amazon", "Google Trust Services", "Cloudflare", "Entrust"
        ]
    def _load_keywords_from_db(self):
        """ä»æ•°æ®åº“åŠ è½½æ•æ„Ÿå…³é”®è¯ï¼Œå¸¦æœ‰ç¼“å­˜æœºåˆ¶"""
        import time
        current_time = time.time()
        
        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
        if self._keyword_cache and (current_time - self._cache_timestamp < self._cache_ttl):
            return self._keyword_cache

        keywords_dict = {}
        try:
            # è¿æ¥æ•°æ®åº“
            connection = pymysql.connect(**DB_CONFIG)
            try:
                with connection.cursor() as cursor:
                    # æ‰§è¡ŒSQLæŸ¥è¯¢
                    sql = """select illegal, (select dict_label from sys_dict_data where dict_type = 'contraband_type' and dict_value = g.category ) as dict_type from gat_illegal_keyword g"""
                    cursor.execute(sql)
                    results = cursor.fetchall()
                    
                    # æ„å»ºå…³é”®è¯å­—å…¸
                    for row in results:
                        keyword = row['illegal']
                        category = row['dict_type']
                        if category not in keywords_dict:
                            keywords_dict[category] = []
                        keywords_dict[category].append(keyword)
            finally:
                connection.close()
            # æ›´æ–°ç¼“å­˜
            self._keyword_cache = keywords_dict
            self._cache_timestamp = current_time
        except Exception as e:
            logger.error(f"ä»æ•°æ®åº“åŠ è½½å…³é”®è¯å¤±è´¥: {e}")
            # å¦‚æœæ•°æ®åº“è¿æ¥å¤±è´¥ï¼Œä»keyword.txtæ–‡ä»¶ä¸­è¯»å–å…³é”®è¯
            try:
                import json
                import os
                # è·å–å½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•
                current_dir = os.path.dirname(os.path.abspath(__file__))
                # æ„å»ºkeyword.txtæ–‡ä»¶çš„å®Œæ•´è·¯å¾„
                keyword_file_path = os.path.join(current_dir, 'keyword.json')
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                if os.path.exists(keyword_file_path):
                    with open(keyword_file_path, 'r', encoding='utf-8') as f:
                        # è¯»å–æ–‡ä»¶å†…å®¹å¹¶è§£æä¸ºå­—å…¸
                        file_content = f.read()
                        # å¤„ç†æ–‡ä»¶å†…å®¹ï¼Œå»é™¤å¯èƒ½çš„BOMå­—ç¬¦
                        file_content = file_content.lstrip('\ufeff')
                        keywords_dict = json.loads(file_content)
                        logger.info(f"æˆåŠŸä»æ–‡ä»¶åŠ è½½å…³é”®è¯ï¼Œå…±åŠ è½½ {len(keywords_dict)} ä¸ªç±»åˆ«")
                else:
                    logger.warning(f"å…³é”®è¯æ–‡ä»¶ä¸å­˜åœ¨: {keyword_file_path}")
            except Exception as file_error:
                logger.error(f"ä»æ–‡ä»¶åŠ è½½å…³é”®è¯å¤±è´¥: {file_error}")
                keywords_dict = {}
        return keywords_dict
    def _extract_subpage_features(self, url):
        """æå–å­é¡µé¢ç‰¹å¾å¹¶è¿›è¡Œæ£€æµ‹"""
        features = {
            'subpage_count': 0,  # æ£€æµ‹çš„å­é¡µé¢æ•°é‡
            'suspicious_subpages': 0,  # å¯ç–‘å­é¡µé¢æ•°é‡
            'avg_subpage_risk': 0.0,  # å­é¡µé¢å¹³å‡é£é™©åˆ†æ•°
            'has_sensitive_subpage': 0,  # æ˜¯å¦åŒ…å«é«˜æ•æ„Ÿå­é¡µé¢
            'subpage_keywords': {},  # å­é¡µé¢ä¸­å‘ç°çš„å…³é”®è¯ç»Ÿè®¡
            'subpage_details': []  # å­é¡µé¢è¯¦ç»†ä¿¡æ¯
        }
        
        try:
            # è·å–ä¸»é¡µé¢å†…å®¹
            response = self.session.get(url, timeout=self.subpage_timeout)
            soup = BeautifulSoup(response.content, 'html.parser')
            parsed_url = urlparse(url)
            base_domain = parsed_url.netloc
            
            # æå–æ‰€æœ‰å†…éƒ¨é“¾æ¥ä½œä¸ºå­é¡µé¢å€™é€‰
            internal_links = set()
            for a_tag in soup.find_all('a', href=True):
                href = a_tag['href'].strip()
                if href and not href.startswith(('javascript:', '#', 'mailto:', 'tel:')):
                    # è½¬æ¢ç›¸å¯¹é“¾æ¥ä¸ºç»å¯¹é“¾æ¥
                    absolute_url = urljoin(url, href)
                    parsed_link = urlparse(absolute_url)
                    
                    # æ£€æŸ¥æ˜¯å¦ä¸ºåŒä¸€åŸŸåä¸‹çš„é“¾æ¥
                    if parsed_link.netloc == base_domain:
                        # æ ‡å‡†åŒ–URLï¼ˆå»é™¤é”šç‚¹ç­‰ï¼‰
                        normalized_url = parsed_link.scheme + '://' + parsed_link.netloc + parsed_link.path
                        if normalized_url not in internal_links and normalized_url != url:
                            internal_links.add(normalized_url)
                            
                            # é™åˆ¶å­é¡µé¢æ•°é‡
                            if len(internal_links) >= self.max_subpages:
                                break
            
            # å¯¹å­é¡µé¢è¿›è¡Œæ£€æµ‹
            features['subpage_count'] = len(internal_links)
            total_risk_score = 0
            
            for subpage_url in internal_links:
                try:
                    # å¯¹å­é¡µé¢è¿›è¡Œç®€å•ç‰¹å¾æå–
                    subpage_features = {}
                    subpage_response = self.session.get(subpage_url, timeout=self.subpage_timeout)
                    subpage_soup = BeautifulSoup(subpage_response.content, 'html.parser')
                    
                    # æå–å­é¡µé¢å†…å®¹ç‰¹å¾
                    text_content = subpage_soup.get_text().lower()
                    
                    # ç»Ÿè®¡æ•æ„Ÿå…³é”®è¯
                    subpage_keyword_count = 0
                    keyword_stats = {}
                    
                    for category, keywords in self.sensitive_keywords.items():
                        count = sum(1 for keyword in keywords if keyword.lower() in text_content)
                        keyword_stats[category] = count
                        subpage_keyword_count += count
                    
                    # è®¡ç®—å­é¡µé¢é£é™©åˆ†æ•°
                    subpage_risk = 0
                    if subpage_keyword_count > 5:
                        subpage_risk = 80  # é«˜é£é™©
                        features['has_sensitive_subpage'] = 1
                    elif subpage_keyword_count > 2:
                        subpage_risk = 50  # ä¸­é£é™©
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç–‘è¡¨å•æˆ–è„šæœ¬
                    has_login_form = 1 if subpage_soup.find('input', type='password') else 0
                    script_count = len(subpage_soup.find_all('script'))
                    
                    if has_login_form and not subpage_url.startswith('https://'):
                        subpage_risk += 30
                    if script_count > 5:
                        subpage_risk += 20
                    
                    # é™åˆ¶é£é™©åˆ†æ•°èŒƒå›´
                    subpage_risk = min(100, max(0, subpage_risk))
                    
                    # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                    total_risk_score += subpage_risk
                    if subpage_risk > 60:
                        features['suspicious_subpages'] += 1
                    
                    # æ›´æ–°å…³é”®è¯ç»Ÿè®¡
                    for category, count in keyword_stats.items():
                        if count > 0:
                            if category not in features['subpage_keywords']:
                                features['subpage_keywords'][category] = 0
                            features['subpage_keywords'][category] += count
                    
                    # ä¿å­˜å­é¡µé¢è¯¦ç»†ä¿¡æ¯
                    features['subpage_details'].append({
                        'url': subpage_url,
                        'risk_score': subpage_risk,
                        'keyword_count': subpage_keyword_count,
                        'has_login_form': has_login_form,
                        'script_count': script_count
                    })
                    
                except Exception as e:
                    logger.warning(f"å­é¡µé¢æ£€æµ‹å¤±è´¥ {subpage_url}: {e}")
                    continue
            
            # è®¡ç®—å¹³å‡é£é™©åˆ†æ•°
            if features['subpage_count'] > 0:
                features['avg_subpage_risk'] = total_risk_score / features['subpage_count']
            
        except Exception as e:
            logger.error(f"å­é¡µé¢ç‰¹å¾æå–å¤±è´¥ {url}: {e}")
        
        return features

    def extract_all_features(self, url):
        """æå–æ‰€æœ‰ç‰¹å¾ï¼ˆåŒ…å«å­é¡µé¢ç‰¹å¾ï¼‰"""
        features = {'url': url}
        
        # æå–å„ç»´åº¦ç‰¹å¾
        domain_features = self._extract_domain_features(url)
        content_features = self._extract_content_features(url)
        network_features = self._extract_network_features(url)
        subpage_features = self._extract_subpage_features(url)  # æ·»åŠ å­é¡µé¢ç‰¹å¾
        
        # åˆå¹¶æ‰€æœ‰ç‰¹å¾
        features.update(domain_features)
        features.update(content_features)
        features.update(network_features)
        features.update(subpage_features)  # æ·»åŠ å­é¡µé¢ç‰¹å¾
        
        return features
    def _load_model(self):
        """åŠ è½½é¢„è®­ç»ƒçš„æœºå™¨å­¦ä¹ æ¨¡å‹"""
        model_path = 'website_detection_model.pkl'
        if os.path.exists(model_path):
            try:
                return joblib.load(model_path)
            except Exception as e:
                logger.warning(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None
    
    def _load_blacklists(self):
        """åŠ è½½é»‘åå•æ•°æ®"""
        try:
            # åŠ è½½å·²çŸ¥æ¶æ„IPåˆ—è¡¨
            if os.path.exists('blacklist_ips.txt'):
                with open('blacklist_ips.txt', 'r') as f:
                    self.blacklisted_ips = {line.strip() for line in f if line.strip()}
            
            # åŠ è½½å·²çŸ¥æ¶æ„åŸŸååˆ—è¡¨
            if os.path.exists('blacklist_domains.txt'):
                with open('blacklist_domains.txt', 'r') as f:
                    self.blacklisted_domains = {line.strip() for line in f if line.strip()}
                    
        except Exception as e:
            logger.warning(f"åŠ è½½é»‘åå•å¤±è´¥: {e}")
    
    def _detect_homograph_attacks(self, domain):
        """æ£€æµ‹åŒå½¢å¼‚ä¹‰å­—ç¬¦æ”»å‡»"""
        homograph_chars = {
            'Ğ°': 'a', 'Ğµ': 'e', 'Ğ¾': 'o', 'Ñ€': 'p', 'Ñ': 'c', 'Ñƒ': 'y', 'Ñ…': 'x',
            'Ğ': 'A', 'Ğ’': 'B', 'Ğ¡': 'C', 'Ğ•': 'E', 'Ğ': 'H', 'Ğš': 'K', 'Ğœ': 'M', 'Ğ': 'O', 'Ğ ': 'P', 'Ğ¢': 'T'
        }
        
        normalized = domain.lower()
        for unicode_char, ascii_char in homograph_chars.items():
            normalized = normalized.replace(unicode_char, ascii_char)
        
        return normalized != domain.lower()
    
    def _calculate_levenshtein_distance(self, s1, s2):
        """è®¡ç®—ç¼–è¾‘è·ç¦»"""
        if len(s1) < len(s2):
            return self._calculate_levenshtein_distance(s2, s1)
        
        if len(s2) == 0:
            return len(s1)
        
        previous_row = list(range(len(s2) + 1))
        for i, c1 in enumerate(s1):
            current_row = [i + 1]
            for j, c2 in enumerate(s2):
                insertions = previous_row[j + 1] + 1
                deletions = current_row[j] + 1
                substitutions = previous_row[j] + (c1 != c2)
                current_row.append(min(insertions, deletions, substitutions))
            previous_row = current_row
        
        return previous_row[-1]
    
    def _extract_domain_features(self, url):
        """æå–åŸŸåç‰¹å¾"""
        features = {}
        try:
            parsed = urlparse(url)
            domain = parsed.netloc
            
            # åŸºç¡€åŸŸåç‰¹å¾
            features['domain_length'] = len(domain)
            features['subdomain_count'] = domain.count('.')
            features['has_hyphen'] = 1 if '-' in domain else 0
            features['has_digits'] = 1 if any(c.isdigit() for c in domain) else 0
            
            # é¡¶çº§åŸŸååˆ†æ
            tld = '.' + domain.split('.')[-1] if '.' in domain else ''
            features['suspicious_tld'] = 1 if tld in self.suspicious_tlds else 0
            
            # å­—ç¬¦åˆ†å¸ƒç‰¹å¾
            features['digit_ratio'] = sum(c.isdigit() for c in domain) / len(domain)
            features['special_char_ratio'] =round( sum(not c.isalnum() for c in domain) / len(domain), 2)
            features['consonant_ratio'] = round( sum(c.lower() in 'bcdfghjklmnpqrstvwxyz' for c in domain) / len(domain), 2)
            
            # ç†µå€¼è®¡ç®—ï¼ˆæ£€æµ‹éšæœºåŸŸåï¼‰
            import math
            char_counts = {}
            for char in domain.lower():
                char_counts[char] = char_counts.get(char, 0) + 1
            entropy = round( -sum((count/len(domain)) * math.log2(count/len(domain)) for count in char_counts.values()), 2)
            features['entropy'] = entropy
            
            # é»‘åå•æ£€æµ‹
            features['in_blacklist'] = 1 if domain in self.blacklisted_domains else 0
            
            # å“ç‰Œé’“é±¼æ£€æµ‹
            brand_similarity = 0
            domain_lower = domain.lower()
            for brand in self.brand_keywords:
                distance = self._calculate_levenshtein_distance(domain_lower, brand)
                similarity = max(0, 1 - distance / max(len(domain_lower), len(brand)))
                brand_similarity = max(brand_similarity, similarity)
            features['brand_similarity'] =round( brand_similarity , 2)
            features['potential_phishing'] = 1 if brand_similarity > 0.7 else 0
            
            # åŒå½¢å¼‚ä¹‰å­—ç¬¦æ”»å‡»æ£€æµ‹
            features['homograph_attack'] = 1 if self._detect_homograph_attacks(domain) else 0
            
            # å¯ç–‘å…³é”®è¯ç»„åˆ
            suspicious_combinations = [
                'login', 'signin', 'verify', 'secure', 'bank', 'update', 'confirm',
                'security', 'account', 'auth', 'password', 'credential'
            ]
            features['suspicious_combo'] = sum(1 for combo in suspicious_combinations if combo in domain.lower())
            
            # WHOISä¿¡æ¯
            try:
                domain_info = whois.whois(domain)
                if domain_info.creation_date:
                    creation_date = domain_info.creation_date
                    if isinstance(creation_date, list):
                        creation_date = creation_date[0]
                    days_since_creation = (datetime.datetime.now() - creation_date).days
                    features['domain_age_days'] = days_since_creation
                    features['is_new_domain'] = 1 if days_since_creation < 30 else 0
                    features['is_very_new_domain'] = 1 if days_since_creation < 7 else 0
                else:
                    features['domain_age_days'] = -1
                    features['is_new_domain'] = 1
                    features['is_very_new_domain'] = 1
                    
                if domain_info.expiration_date:
                    expiration_date = domain_info.expiration_date
                    if isinstance(expiration_date, list):
                        expiration_date = expiration_date[0]
                    days_to_expire = (expiration_date - datetime.datetime.now()).days
                    features['days_to_expire'] = days_to_expire
                    features['short_registration'] = 1 if days_to_expire < 365 else 0
                else:
                    features['days_to_expire'] = -1
                    features['short_registration'] = 1
                    
                # æ³¨å†Œå•†ä¿¡æ¯
                registrar = str(domain_info.registrar).lower() if domain_info.registrar else ''
                suspicious_registrars = ['namecheap', 'godaddy', 'publicdomainregistry']
                features['suspicious_registrar'] = 1 if any(r in registrar for r in suspicious_registrars) else 0
                    
            except Exception as e:
                features['domain_age_days'] = -1
                features['is_new_domain'] = 0  # ä¿®æ”¹ä¸º0ï¼Œä¸é»˜è®¤ä¸ºæ–°åŸŸå
                features['is_very_new_domain'] = 0  # ä¿®æ”¹ä¸º0ï¼Œä¸é»˜è®¤ä¸ºææ–°åŸŸå
                features['days_to_expire'] = -1
                features['short_registration'] = 0  # ä¿®æ”¹ä¸º0ï¼Œä¸é»˜è®¤ä¸ºçŸ­æœŸæ³¨å†Œ
                features['suspicious_registrar'] = 0
                
        except Exception as e:
            logger.error(f"åŸŸåç‰¹å¾æå–å¤±è´¥ {url}: {e}")
            
        return features
    
    def _extract_content_features(self, url):
        """æå–å†…å®¹ç‰¹å¾"""
        features = {}
        try:
            response = self.session.get(url, timeout=self.timeout)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # åŸºç¡€å†…å®¹ç‰¹å¾
            features['content_length'] = len(response.content)
            features['text_length'] = len(soup.get_text())
            features['image_count'] = len(soup.find_all('img'))
            features['link_count'] = len(soup.find_all('a'))
            features['form_count'] = len(soup.find_all('form'))
            features['external_links'] = len([a for a in soup.find_all('a', href=True) 
                                            if a['href'].startswith('http') and urlparse(url).netloc not in a['href']])
            
            # æ•æ„Ÿå…³é”®è¯æ£€æµ‹ - åˆ†ç±»ç»Ÿè®¡
            text_content = soup.get_text().lower()
            total_sensitive = 0
            for category, keywords in self.sensitive_keywords.items():
                category_count = sum(1 for keyword in keywords if keyword.lower() in text_content)
                features[f'sensitive_{category}'] = category_count
                total_sensitive += category_count
            
            features['sensitive_keyword_count'] = total_sensitive
            features['sensitive_keyword_ratio'] = round(total_sensitive / max(len(text_content.split()), 1), 2)
            
            # é¡µé¢è´¨é‡æŒ‡æ ‡
            features['has_title'] = 1 if soup.title and soup.title.string else 0
            features['title_length'] = len(soup.title.string) if soup.title and soup.title.string else 0
            features['has_description'] = 1 if soup.find('meta', attrs={'name': 'description'}) else 0
            features['has_keywords'] = 1 if soup.find('meta', attrs={'name': 'keywords'}) else 0
            features['has_robots'] = 1 if soup.find('meta', attrs={'name': 'robots'}) else 0
            
            # é¡µé¢ç»“æ„åˆ†æ
            features['has_login_form'] = 1 if soup.find('input', type='password') else 0
            features['has_contact_info'] = 1 if any(keyword in text_content for keyword in ['è”ç³»æˆ‘ä»¬', 'contact', 'ç”µè¯', 'é‚®ç®±']) else 0
            features['has_privacy_policy'] = 1 if any(keyword in text_content for keyword in ['éšç§æ”¿ç­–', 'privacy', 'æ¡æ¬¾']) else 0
            
            # å›¾ç‰‡è´¨é‡åˆ†æ
            suspicious_images = 0
            for img in soup.find_all('img'):
                src = img.get('src', '')
                if not src or src.startswith('data:'):
                    suspicious_images += 1
                elif 'logo' in src.lower() or 'banner' in src.lower():
                    continue
            features['suspicious_images'] = suspicious_images
            
            # è„šæœ¬åˆ†æ
            scripts = soup.find_all('script')
            features['script_count'] = len(scripts)
            suspicious_scripts = 0
            for script in scripts:
                if script.string and any(keyword in script.string.lower() for keyword in ['eval', 'document.write', 'unescape']):
                    suspicious_scripts += 1
            features['suspicious_scripts'] = suspicious_scripts
            
            # é‡å®šå‘æ£€æµ‹
            if response.history:
                features['redirect_count'] = len(response.history)
                features['final_url'] = response.url
                features['domain_changed'] = 1 if urlparse(url).netloc != urlparse(response.url).netloc else 0
            else:
                features['redirect_count'] = 0
                features['final_url'] = url
                features['domain_changed'] = 0
            
            # SSLè¯ä¹¦ä¿¡æ¯ - å¢å¼ºç‰ˆ
            try:
                parsed = urlparse(url)
                context = ssl.create_default_context()
                with socket.create_connection((parsed.netloc, 443), timeout=5) as sock:
                    with context.wrap_socket(sock, server_hostname=parsed.netloc) as ssock:
                        cert = ssock.getpeercert()
                        features['has_ssl'] = 1
                        features['ssl_valid'] = 1 if cert else 0
                        
                        if cert:
                            # æ£€æŸ¥è¯ä¹¦é¢å‘è€…
                            issuer = dict(x[0] for x in cert['issuer'])
                            ca_name = issuer.get('organizationName', '')
                            features['trusted_ca'] = 1 if any(ca in ca_name for ca in self.trusted_cas) else 0
                            
                            # æ£€æŸ¥è¯ä¹¦æœ‰æ•ˆæœŸ
                            not_before = datetime.datetime.strptime(cert['notBefore'], '%b %d %H:%M:%S %Y %Z')
                            not_after = datetime.datetime.strptime(cert['notAfter'], '%b %d %H:%M:%S %Y %Z')
                            features['cert_valid_days'] = (not_after - datetime.datetime.now()).days
                            features['cert_too_new'] = 1 if (datetime.datetime.now() - not_before).days < 7 else 0
                            
                            # åŸŸååŒ¹é…æ£€æŸ¥
                            cn = None
                            for item in cert['subject']:
                                for key, value in item:
                                    if key == 'commonName':
                                        cn = value
                                        break
                            
                            features['ssl_domain_match'] = 1 if cn and parsed.netloc in cn else 0
                            features['wildcard_cert'] = 1 if cn and '*' in cn else 0
                            
            except Exception as e:
                features['has_ssl'] = 0
                features['ssl_valid'] = 0
                features['trusted_ca'] = 0
                features['cert_valid_days'] = -1
                features['cert_too_new'] = 0
                features['ssl_domain_match'] = 0
                features['wildcard_cert'] = 0
                
        except Exception as e:
            # logger.error(f"å†…å®¹ç‰¹å¾æå–å¤±è´¥ {url}: {e}")
            color_printer.print(f"ğŸš¨ å†…å®¹ç‰¹å¾æå–å¤±è´¥ {url}: {e}", 'red', bold=True)
            features.update({
                'content_length': 0, 'text_length': 0, 'image_count': 0,
                'link_count': 0, 'form_count': 0, 'external_links': 0,
                'sensitive_keyword_count': 0, 'sensitive_keyword_ratio': 0,
                'has_title': 0, 'title_length': 0, 'has_description': 0,
                'has_keywords': 0, 'has_robots': 0, 'has_login_form': 0,
                'has_contact_info': 0, 'has_privacy_policy': 0, 'suspicious_images': 0,
                'script_count': 0, 'suspicious_scripts': 0, 'redirect_count': 0,
                'domain_changed': 0, 'has_ssl': 0, 'ssl_valid': 0, 'trusted_ca': 0,
                'cert_valid_days': -1, 'cert_too_new': 0, 'ssl_domain_match': 0,
                'wildcard_cert': 0
            })
            for category in self.sensitive_keywords.keys():
                features[f'sensitive_{category}'] = 0
            
        return features
    
    def _extract_network_features(self, url):
        """æå–ç½‘ç»œç‰¹å¾"""
        features = {}
        try:
            parsed = urlparse(url)
            domain = parsed.netloc
            
            # DNSè§£æ - å¢å¼ºç‰ˆ
            try:
                # Aè®°å½•
                answers = dns.resolver.resolve(domain, 'A')
                features['dns_resolved'] = 1
                features['ip_count'] = len(answers)
                features['first_ip'] = str(answers[0])
                
                # é»‘åå•IPæ£€æŸ¥
                features['blacklisted_ip'] = 1 if features['first_ip'] in self.blacklisted_ips else 0
                
                # MXè®°å½•
                try:
                    mx_answers = dns.resolver.resolve(domain, 'MX')
                    features['has_mx'] = 1
                    features['mx_count'] = len(mx_answers)
                except:
                    features['has_mx'] = 0
                    features['mx_count'] = 0
                
                # TXTè®°å½•ï¼ˆSPFæ£€æŸ¥ï¼‰
                try:
                    txt_answers = dns.resolver.resolve(domain, 'TXT')
                    spf_records = [str(record) for record in txt_answers if 'spf' in str(record).lower()]
                    features['has_spf'] = 1 if spf_records else 0
                except:
                    features['has_spf'] = 0
                    
            except:
                features['dns_resolved'] = 0
                features['ip_count'] = 0
                features['first_ip'] = ''
                features['has_mx'] = 0
                features['mx_count'] = 0
                features['has_spf'] = 0
                features['blacklisted_ip'] = 0
            
            # å“åº”æ—¶é—´åˆ†æ
            start_time = time.time()
            try:
                response = self.session.head(url, timeout=5)
                features['response_time'] =round(time.time() - start_time, 2)
                features['http_status'] = response.status_code
                features['web_accessible'] = 1
                
                # æœåŠ¡å™¨ä¿¡æ¯
                features['server_header'] = response.headers.get('Server', '')
                features['powered_by'] = response.headers.get('X-Powered-By', '')
                
                # å®‰å…¨å¤´æ£€æŸ¥
                security_headers = {
                    'strict-transport-security': 'hsts',
                    'x-frame-options': 'x_frame_options',
                    'x-content-type-options': 'x_content_type',
                    'x-xss-protection': 'x_xss_protection',
                    'content-security-policy': 'csp'
                }
                
                for header, feature_name in security_headers.items():
                    features[feature_name] = 1 if header in response.headers else 0
                
            except:
                features['response_time'] = -1
                features['http_status'] = 0
                features['web_accessible'] = 0
                features['server_header'] = ''
                features['powered_by'] = ''
                features['hsts'] = 0
                features['x_frame_options'] = 0
                features['x_content_type'] = 0
                features['x_xss_protection'] = 0
                features['csp'] = 0
                features['blacklisted_ip'] = 0
                
        except Exception as e:
            logger.error(f"ç½‘ç»œç‰¹å¾æå–å¤±è´¥ {url}: {e}")
            
        return features
    
    
    
    def predict_risk(self, features):
        """é¢„æµ‹é£é™©ç­‰çº§ - å¢å¼ºç‰ˆè¯„åˆ†ç®—æ³•"""
        if not self.model:
            # å¢å¼ºçš„åŸºäºè§„åˆ™é£é™©è¯„åˆ†
            risk_score = 0
            # æ·»åŠ å­é¡µé¢é£é™©å› å­
            if features.get('has_sensitive_subpage', 0) == 1:
                risk_score += 30  # åŒ…å«é«˜æ•æ„Ÿå­é¡µé¢
            if features.get('suspicious_subpages', 0) > 0:
                risk_score += features['suspicious_subpages'] * 10  # æ¯ä¸ªå¯ç–‘å­é¡µé¢å¢åŠ é£é™©
            if features.get('avg_subpage_risk', 0) > 50:
                risk_score += 15  # å­é¡µé¢å¹³å‡é£é™©è¾ƒé«˜
            # åŸŸåé£é™©å› å­ï¼ˆæƒé‡å¢åŠ ï¼‰
            if features.get('in_blacklist', 0) == 1:
                risk_score += 50  # é»‘åå•ç›´æ¥é«˜åˆ†
            if features.get('homograph_attack', 0) == 1:
                risk_score += 30  # åŒå½¢å¼‚ä¹‰å­—ç¬¦æ”»å‡»
            if features.get('potential_phishing', 0) == 1:
                risk_score += 25  # å“ç‰Œé’“é±¼
            if features.get('brand_similarity', 0) > 0.8:
                risk_score += 20  # é«˜å“ç‰Œç›¸ä¼¼åº¦
            if features.get('entropy', 0) > 4.0:
                risk_score += 15  # é«˜ç†µå€¼ï¼ˆéšæœºåŸŸåï¼‰
            if features.get('is_very_new_domain', 0) == 1:
                risk_score += 5  # éå¸¸æ–°çš„åŸŸå
            if features.get('short_registration', 0) == 1:
                risk_score += 5  # çŸ­æœŸæ³¨å†Œ
            if features.get('suspicious_registrar', 0) == 1:
                risk_score += 10  # å¯ç–‘æ³¨å†Œå•†
            if features.get('suspicious_combo', 0) > 2:
                risk_score += 15  # å¯ç–‘å…³é”®è¯ç»„åˆ
            
            # å†…å®¹é£é™©å› å­ï¼ˆç»†åŒ–åˆ†ç±»ï¼‰
            gambling_content = features.get('sensitive_gambling', 0)
            fraud_content = features.get('sensitive_fraud', 0)
            porn_content = features.get('sensitive_pornography', 0)
            financial_fraud = features.get('sensitive_financial_fraud', 0)
            
            total_sensitive = gambling_content + fraud_content + porn_content + financial_fraud
            if total_sensitive > 10:
                risk_score += 30
            elif total_sensitive > 5:
                risk_score += 20
            elif total_sensitive > 0:
                risk_score += 10
                
            if features.get('sensitive_keyword_ratio', 0) > 0.1:
                risk_score += 15
            if features.get('has_login_form', 0) == 1 and features.get('has_ssl', 0) == 0:
                risk_score += 25  # ç™»å½•è¡¨å•æ— SSL
            if features.get('suspicious_scripts', 0) > 3:
                risk_score += 15  # å¯ç–‘è„šæœ¬
            if features.get('domain_changed', 0) == 1:
                risk_score += 20  # åŸŸåè·³è½¬
                
            # SSLè¯ä¹¦é£é™©å› å­
            if features.get('has_ssl', 0) == 0:
                risk_score += 15
            if features.get('ssl_valid', 0) == 0:
                risk_score += 20
            if features.get('trusted_ca', 0) == 0:
                risk_score += 10  # éå¯ä¿¡CA
            if features.get('cert_too_new', 0) == 1:
                risk_score += 10  # è¯ä¹¦å¤ªæ–°
            if features.get('cert_valid_days', 0) < 30:
                risk_score += 10  # è¯ä¹¦å³å°†è¿‡æœŸ
                
            # ç½‘ç»œé£é™©å› å­
            if features.get('blacklisted_ip', 0) == 1:
                risk_score += 40  # é»‘åå•IP
            if features.get('web_accessible', 0) == 0:
                risk_score += 30  # æ— æ³•è®¿é—®
            if features.get('dns_resolved', 0) == 0:
                risk_score += 25
            if features.get('response_time', 0) > 5:
                risk_score += 10  # å“åº”æ—¶é—´è¿‡é•¿
            if features.get('http_status', 0) >= 400:
                risk_score += 15  # HTTPé”™è¯¯çŠ¶æ€
                
            # å®‰å…¨å¤´æ£€æŸ¥ï¼ˆè´Ÿå‘é£é™©ï¼‰
            security_score = (features.get('hsts', 0) + features.get('x_frame_options', 0) + 
                            features.get('x_content_type', 0) + features.get('x_xss_protection', 0) + 
                            features.get('csp', 0))
            risk_score -= security_score * 2  # å®‰å…¨å¤´å‡å°‘é£é™©
            
            # ä¿¡ä»»æŒ‡æ ‡ï¼ˆè´Ÿå‘é£é™©ï¼‰
            if features.get('has_contact_info', 0) == 1:
                risk_score -= 10
            if features.get('has_privacy_policy', 0) == 1:
                risk_score -= 10
            if features.get('has_mx', 0) == 1:
                risk_score -= 5  # æœ‰MXè®°å½•
            if features.get('domain_age_days', 0) > 365:
                risk_score -= 15  # è€åŸŸå
                
            # é£é™©ç­‰çº§åˆ¤å®šï¼ˆè°ƒæ•´é˜ˆå€¼ï¼‰
            risk_score = max(0, min(100, risk_score))  # é™åˆ¶åœ¨0-100èŒƒå›´å†…
            
            if risk_score >= 70:
                return 'HIGH', risk_score
            elif risk_score >= 40:
                return 'MEDIUM', risk_score
            else:
                return 'LOW', risk_score
        else:
            # ä½¿ç”¨æœºå™¨å­¦ä¹ æ¨¡å‹é¢„æµ‹
            feature_vector = self._prepare_features_for_model(features)
            prediction = self.model.predict([feature_vector])[0]
            probability = self.model.predict_proba([feature_vector])[0]
            risk_score = int(probability[1] * 100) if len(probability) > 1 else 50
            
            if prediction == 1:
                return 'HIGH' if risk_score > 70 else 'MEDIUM', risk_score
            else:
                return 'LOW', risk_score
    
    def _prepare_features_for_model(self, features):
        """å‡†å¤‡æœºå™¨å­¦ä¹ æ¨¡å‹éœ€è¦çš„ç‰¹å¾å‘é‡"""
        feature_order = [
            'domain_length', 'subdomain_count', 'has_hyphen', 'has_digits',
            'suspicious_tld', 'digit_ratio', 'special_char_ratio',
            'domain_age_days', 'is_new_domain', 'days_to_expire',
            'content_length', 'text_length', 'image_count', 'link_count',
            'form_count', 'sensitive_keyword_count', 'sensitive_keyword_ratio',
            'has_title', 'has_description', 'has_keywords', 'has_ssl',
            'ssl_valid', 'ssl_domain_match', 'dns_resolved', 'ip_count',
            'response_time', 'http_status'
        ]
        
        return [features.get(key, 0) for key in feature_order]

class BatchDetector:
    """æ‰¹é‡æ£€æµ‹å™¨"""
    
    def __init__(self, max_workers=10):
        self.detector = WebsiteDetector()
        self.max_workers = max_workers
        self.results = []
    
    def detect_single(self, url):
        """æ£€æµ‹å•ä¸ªURL"""
        try:
            # logger.info(f"å¼€å§‹æ£€æµ‹: {url}")
            color_printer.print(f"ğŸš€ å¼€å§‹æ£€æµ‹ {url} ", 'cyan', bold=True)
            # æ ‡å‡†åŒ–URL
            if not url.startswith(('http://', 'https://')):
                url = 'http://' + url
            
            # æå–ç‰¹å¾
            features = self.detector.extract_all_features(url)
            
            # é¢„æµ‹é£é™©
            risk_level, risk_score = self.detector.predict_risk(features)
            
            # é£é™©ç­‰çº§ä¸­æ–‡æ˜ å°„
            risk_level_cn = {
                'HIGH': 'é«˜é£é™©',
                'MEDIUM': 'ä¸­é£é™©', 
                'LOW': 'ä½é£é™©',
                'ERROR': 'æ£€æµ‹å¤±è´¥'
            }.get(risk_level, risk_level)
            
            # ç”Ÿæˆä¸­æ–‡é£é™©æè¿°
            risk_description = self._generate_risk_description(features, risk_level, risk_score)
            
            result = {
                'ç½‘å€': url,
                'é£é™©ç­‰çº§': risk_level_cn,
                'é£é™©è¯„åˆ†': f"{risk_score}%",
                'é£é™©æè¿°': risk_description,
                'æ£€æµ‹æ—¶é—´': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'è¯¦ç»†ç‰¹å¾': self._translate_features(features),
                'è‹±æ–‡åŸæ–‡': {
                    'url': url,
                    'risk_level': risk_level,
                    'risk_score': risk_score,
                    'features': features,
                    'timestamp': datetime.datetime.now().isoformat()
                }
            }
            
            # æ ¹æ®é£é™©ç­‰çº§è®¾ç½®ä¸åŒé¢œè‰²
            if risk_level_cn == "é«˜é£é™©":
                color = 'red'
            elif risk_level_cn == "ä¸­é£é™©":
                color = 'yellow'
            else:  # ä½é£é™©
                color = 'blue'
                
            color_printer.print(f"æ£€æµ‹å®Œæˆ: {url} - é£é™©ç­‰çº§: {risk_level_cn} ({risk_score}%) - é£é™©æè¿°ï¼š {risk_description} \n", color, bold=True)
            return result
            
        except Exception as e:
            
            color_printer.print(f"ğŸš¨ æ£€æµ‹å¤±è´¥ {url}: {e}", 'red', bold=True)
            return {
                'ç½‘å€': url,
                'é£é™©ç­‰çº§': 'æ£€æµ‹å¤±è´¥',
                'é£é™©è¯„åˆ†': '0%',
                'é£é™©æè¿°': f'æ£€æµ‹è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}',
                'æ£€æµ‹æ—¶é—´': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'è¯¦ç»†ç‰¹å¾': {},
                'é”™è¯¯ä¿¡æ¯': str(e)
            }
    
    def _generate_risk_description(self, features, risk_level, risk_score):
        """ç”Ÿæˆä¸­æ–‡é£é™©æè¿°"""
        descriptions = []
        
        if risk_level == 'HIGH':
            descriptions.append("ğŸš¨ è¯¥ç½‘ç«™å­˜åœ¨ä¸¥é‡å®‰å…¨é£é™©")
        elif risk_level == 'MEDIUM':
            descriptions.append("âš ï¸ è¯¥ç½‘ç«™å­˜åœ¨ä¸€å®šå®‰å…¨é£é™©")
        elif risk_level == 'LOW':
            descriptions.append("âœ… è¯¥ç½‘ç«™ç›¸å¯¹å®‰å…¨")
        
        # åŸŸåç›¸å…³é£é™©
        if features.get('in_blacklist', 0) == 1:
            descriptions.append("â€¢ åŸŸååœ¨å·²çŸ¥æ¶æ„åŸŸåé»‘åå•ä¸­")
        if features.get('homograph_attack', 0) == 1:
            descriptions.append("â€¢ æ£€æµ‹åˆ°åŒå½¢å¼‚ä¹‰å­—ç¬¦æ”»å‡»ï¼ˆé’“é±¼åŸŸåï¼‰")
        if features.get('potential_phishing', 0) == 1:
            descriptions.append("â€¢ ç–‘ä¼¼å“ç‰Œé’“é±¼ç½‘ç«™")
        if features.get('is_very_new_domain', 0) == 1:
            descriptions.append("â€¢ åŸŸåæ³¨å†Œæ—¶é—´æçŸ­ï¼ˆ7å¤©å†…ï¼‰(whoisè¯·æ±‚å¯èƒ½å­˜åœ¨ç½‘ç»œå¼‚å¸¸)")
        if features.get('short_registration', 0) == 1:
            descriptions.append("â€¢ åŸŸåæ³¨å†ŒæœŸé™è¿‡çŸ­ï¼ˆå°‘äº1å¹´ï¼‰(whoisè¯·æ±‚å¯èƒ½å­˜åœ¨è¯·æ±‚ç½‘ç»œå¼‚å¸¸)")
            
        # å†…å®¹é£é™©
        total_sensitive = (features.get('sensitive_gambling', 0) + 
                          features.get('sensitive_fraud', 0) + 
                          features.get('sensitive_pornography', 0) + 
                          features.get('sensitive_financial_fraud', 0))
        
        if total_sensitive > 0:
            if features.get('sensitive_gambling', 0) > 0:
                descriptions.append("â€¢ åŒ…å«èµŒåšç›¸å…³å†…å®¹")
            if features.get('sensitive_fraud', 0) > 0:
                descriptions.append("â€¢ åŒ…å«è¯ˆéª—ç›¸å…³å†…å®¹")
            if features.get('sensitive_pornography', 0) > 0:
                descriptions.append("â€¢ åŒ…å«è‰²æƒ…ç›¸å…³å†…å®¹")
            if features.get('sensitive_financial_fraud', 0) > 0:
                descriptions.append("â€¢ åŒ…å«é‡‘èè¯ˆéª—ç›¸å…³å†…å®¹")
                
        # SSLè¯ä¹¦é£é™©
        if features.get('has_ssl', 0) == 0:
            descriptions.append("â€¢ ç½‘ç«™æœªå¯ç”¨HTTPSåŠ å¯†")
        elif features.get('ssl_valid', 0) == 0:
            descriptions.append("â€¢ SSLè¯ä¹¦æ— æ•ˆæˆ–å·²è¿‡æœŸ")
        elif features.get('trusted_ca', 0) == 0:
            descriptions.append("â€¢ SSLè¯ä¹¦é¢å‘æœºæ„ä¸å—ä¿¡ä»»")
            
        # ç½‘ç»œé£é™©
        if features.get('blacklisted_ip', 0) == 1:
            descriptions.append("â€¢ æœåŠ¡å™¨IPåœ°å€åœ¨é»‘åå•ä¸­")
        if features.get('web_accessible', 0) == 0:
            descriptions.append("â€¢ ç½‘ç«™æ— æ³•è®¿é—®")
        if features.get('response_time', 0) > 5:
            descriptions.append("â€¢ ç½‘ç«™å“åº”é€Ÿåº¦è¿‡æ…¢")

        # æ·»åŠ å­é¡µé¢é£é™©æè¿°
        if features.get('has_sensitive_subpage', 0) == 1:
            descriptions.append("â€¢ å­é¡µé¢ä¸­å‘ç°é«˜æ•æ„Ÿå†…å®¹")
        if features.get('suspicious_subpages', 0) > 0:
            descriptions.append(f"â€¢ å‘ç° {features['suspicious_subpages']} ä¸ªå¯ç–‘å­é¡µé¢")
        subpage_keywords = features.get('subpage_keywords', {})
        if isinstance(subpage_keywords, dict) and subpage_keywords:
            for category, count in features['subpage_keywords'].items():
                category_map = {
                    'gambling': 'èµŒåš',
                    'pornography': 'è‰²æƒ…', 
                    'fraud': 'è¯ˆéª—',
                    'illegal_trade': 'éæ³•äº¤æ˜“',
                    'cybercrime': 'ç½‘ç»œçŠ¯ç½ª',
                    'financial_fraud': 'é‡‘èè¯ˆéª—'
                }
                category_name = category_map.get(category, category)
                descriptions.append(f"â€¢ å­é¡µé¢åŒ…å« {count} ä¸ª{category_name}ç›¸å…³å…³é”®è¯") 
            
        # å®‰å…¨å»ºè®®
        if risk_level in ['HIGH', 'MEDIUM']:
            descriptions.append("\nğŸ’¡ å»ºè®®ï¼šè¯·å‹¿åœ¨æ­¤ç½‘ç«™è¾“å…¥ä¸ªäººä¿¡æ¯æˆ–è¿›è¡Œä»»ä½•äº¤æ˜“")
        else:
            descriptions.append("\nğŸ’¡ å»ºè®®ï¼šç½‘ç«™ç›¸å¯¹å®‰å…¨ï¼Œä½†ä»éœ€ä¿æŒè­¦æƒ•")
            
        return '\n'.join(descriptions)
    
    def _translate_features(self, features):
        """ç¿»è¯‘ç‰¹å¾åç§°ä¸ºä¸­æ–‡ï¼ˆåŒ…å«å­é¡µé¢ç‰¹å¾ï¼‰"""
        if not isinstance(features, dict):
            return {}
            
        translate_map = {
            'domain_length': 'åŸŸåé•¿åº¦',
            'subdomain_count': 'å­åŸŸåæ•°é‡',
            'has_hyphen': 'åŒ…å«è¿å­—ç¬¦',
            'has_digits': 'åŒ…å«æ•°å­—',
            'suspicious_tld': 'å¯ç–‘é¡¶çº§åŸŸå',
            'digit_ratio': 'æ•°å­—æ¯”ä¾‹',
            'special_char_ratio': 'ç‰¹æ®Šå­—ç¬¦æ¯”ä¾‹',
            'consonant_ratio': 'è¾…éŸ³æ¯”ä¾‹',
            'entropy': 'ç†µå€¼ï¼ˆéšæœºæ€§ï¼‰',
            'in_blacklist': 'é»‘åå•åŒ¹é…',
            'brand_similarity': 'å“ç‰Œç›¸ä¼¼åº¦',
            'potential_phishing': 'ç–‘ä¼¼é’“é±¼',
            'homograph_attack': 'åŒå½¢å¼‚ä¹‰æ”»å‡»',
            'suspicious_combo': 'å¯ç–‘å…³é”®è¯ç»„åˆ',
            'domain_age_days': 'åŸŸåå¹´é¾„ï¼ˆå¤©ï¼‰',
            'is_new_domain': 'æ–°åŸŸåï¼ˆ30å¤©å†…ï¼‰',
            'is_very_new_domain': 'ææ–°åŸŸåï¼ˆ7å¤©å†…ï¼‰',
            'days_to_expire': 'åˆ°æœŸå‰©ä½™å¤©æ•°',
            'short_registration': 'çŸ­æœŸæ³¨å†Œ',
            'suspicious_registrar': 'å¯ç–‘æ³¨å†Œå•†',
            'content_length': 'å†…å®¹é•¿åº¦',
            'text_length': 'æ–‡æœ¬é•¿åº¦',
            'image_count': 'å›¾ç‰‡æ•°é‡',
            'link_count': 'é“¾æ¥æ•°é‡',
            'form_count': 'è¡¨å•æ•°é‡',
            'external_links': 'å¤–éƒ¨é“¾æ¥æ•°',
            'sensitive_gambling': 'èµŒåšå…³é”®è¯',
            'sensitive_fraud': 'è¯ˆéª—å…³é”®è¯',
            'sensitive_pornography': 'è‰²æƒ…å…³é”®è¯',
            'sensitive_financial_fraud': 'é‡‘èè¯ˆéª—å…³é”®è¯',
            'sensitive_illegal_trade': 'éæ³•äº¤æ˜“å…³é”®è¯',
            'sensitive_cybercrime': 'ç½‘ç»œçŠ¯ç½ªå…³é”®è¯',
            'sensitive_è¿è§„ä¹¦ç±': 'è¿è§„ä¹¦ç±å…³é”®è¯æ•°é‡',
            'sensitive_ç½‘ç«™è¿ç¦è¯': 'ç½‘ç«™è¿ç¦è¯æ•°é‡',
            'sensitive_æ¶‰ç¨³': 'æ¶‰ç¨³å…³é”®è¯æ•°é‡',
            'sensitive_æ¶‰é»„': 'æ¶‰é»„å…³é”®è¯æ•°é‡',
            'sensitive_æ¶‰èµŒ': 'æ¶‰èµŒå…³é”®è¯æ•°é‡',
            'sensitive_æ¶‰æ”¿': 'æ¶‰æ”¿å…³é”®è¯æ•°é‡',
            'sensitive_æ¶‰æªæš´': 'æ¶‰æªæš´å…³é”®è¯æ•°é‡',
            'sensitive_æ¶‰ææ¶‰é‚ª': 'æ¶‰ææ¶‰é‚ªå…³é”®è¯æ•°é‡',
            'sensitive_æ¶‰é»‘ç°äº§': 'æ¶‰é»‘ç°äº§å…³é”®è¯æ•°é‡',
            'sensitive_æ¶‰ç”µè¯ˆ': 'æ¶‰ç”µè¯ˆå…³é”®è¯æ•°é‡',
            'sensitive_è¿è§„åŒ–å­¦å“': 'è¿è§„åŒ–å­¦å“å…³é”®è¯æ•°é‡',
            'sensitive_keyword_count': 'æ•æ„Ÿè¯æ€»æ•°',
            'sensitive_keyword_ratio': 'æ•æ„Ÿè¯å æ¯”',
            'sensitive_keyword_count': 'æ•æ„Ÿè¯æ€»æ•°',
            'sensitive_keyword_ratio': 'æ•æ„Ÿè¯å æ¯”',
            'has_title': 'æœ‰æ ‡é¢˜',
            'title_length': 'æ ‡é¢˜é•¿åº¦',
            'has_description': 'æœ‰æè¿°',
            'has_keywords': 'æœ‰å…³é”®è¯',
            'has_robots': 'æœ‰robots',
            'has_login_form': 'æœ‰ç™»å½•è¡¨å•',
            'has_contact_info': 'æœ‰è”ç³»ä¿¡æ¯',
            'has_privacy_policy': 'æœ‰éšç§æ”¿ç­–',
            'suspicious_images': 'å¯ç–‘å›¾ç‰‡',
            'script_count': 'è„šæœ¬æ•°é‡',
            'suspicious_scripts': 'å¯ç–‘è„šæœ¬',
            'redirect_count': 'é‡å®šå‘æ¬¡æ•°',
            'domain_changed': 'åŸŸåå˜æ›´',
            'has_ssl': 'æœ‰SSLè¯ä¹¦',
            'ssl_valid': 'SSLæœ‰æ•ˆ',
            'trusted_ca': 'å¯ä¿¡CA',
            'cert_valid_days': 'è¯ä¹¦æœ‰æ•ˆå¤©æ•°',
            'cert_too_new': 'è¯ä¹¦å¤ªæ–°',
            'ssl_domain_match': 'åŸŸååŒ¹é…',
            'wildcard_cert': 'é€šé…ç¬¦è¯ä¹¦',
            'dns_resolved': 'DNSè§£ææˆåŠŸ',
            'ip_count': 'IPæ•°é‡',
            'first_ip': 'é¦–ä¸ªIP',
            'blacklisted_ip': 'IPé»‘åå•',
            'has_mx': 'æœ‰MXè®°å½•',
            'mx_count': 'MXè®°å½•æ•°',
            'has_spf': 'æœ‰SPFè®°å½•',
            'web_accessible': 'å¯è®¿é—®',
            'response_time': 'å“åº”æ—¶é—´',
            'http_status': 'HTTPçŠ¶æ€ç ',
            'server_header': 'æœåŠ¡å™¨ä¿¡æ¯',
            'powered_by': 'æŠ€æœ¯æ ˆ',
            'hsts': 'HSTSå®‰å…¨å¤´',
            'x_frame_options': 'X-Frame-Options',
            'x_content_type': 'X-Content-Type-Options',
            'x_xss_protection': 'X-XSS-Protection',
            'csp': 'Content-Security-Policy',
            'subpage_count': 'æ£€æµ‹å­é¡µé¢æ•°é‡',
            'suspicious_subpages': 'å¯ç–‘å­é¡µé¢æ•°',
            'avg_subpage_risk': 'å­é¡µé¢å¹³å‡é£é™©',
            'has_sensitive_subpage': 'åŒ…å«æ•æ„Ÿå­é¡µé¢',
            'subpage_keywords': 'å­é¡µé¢ä¸­å‘ç°çš„å…³é”®è¯ç»Ÿè®¡',
            'subpage_details': 'å­é¡µé¢è¯¦ç»†ä¿¡æ¯'
        }
        translated = {}
        for key, value in features.items():
            if key in translate_map:
                chinese_key = translate_map[key]
                if isinstance(value, (int, float)) and value != -1:
                    translated[chinese_key] = value
                elif value not in [None, '', -1]:
                    translated[chinese_key] = value
            elif key == 'url':
                translated['ç½‘å€'] = value
            elif key == 'final_url':
                translated['æœ€ç»ˆç½‘å€'] = value
                
        return translated
    
    def detect_batch(self, urls):
        """æ‰¹é‡æ£€æµ‹"""
        self.results = []
        total = len(urls)
        
        logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡æ£€æµ‹ï¼Œå…± {total} ä¸ªç½‘ç«™")
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_url = {executor.submit(self.detect_single, url): url for url in urls}
            
            for i, future in enumerate(as_completed(future_to_url), 1):
                result = future.result()
                self.results.append(result)
                
                # è·å–ä¸­æ–‡é£é™©ç­‰çº§ç”¨äºè¿›åº¦æ˜¾ç¤º
                risk_level = result.get('é£é™©ç­‰çº§', 'æœªçŸ¥')
                url = result.get('ç½‘å€', 'æœªçŸ¥ç½‘å€')
                
                # è¿›åº¦æ˜¾ç¤º
                progress_bar = self._create_progress_bar(i, total)
                color_printer.print(f"{progress_bar} {i}/{total} - {url} - {risk_level}", 'cyan', bold=True)
        
        # ç”Ÿæˆä¸­æ–‡ç»Ÿè®¡æ‘˜è¦
        stats = self._generate_chinese_summary(self.results)
        logger.info(stats)
        
        return self.results
    
    def _create_progress_bar(self, current, total, length=20):
        """åˆ›å»ºè¿›åº¦æ¡"""
        progress = current / total
        filled = int(length * progress)
        bar = 'â–ˆ' * filled + 'â–‘' * (length - filled)
        return f"[{bar}] {progress*100:.1f}%"
    
    def _generate_chinese_summary(self, results):
        """ç”Ÿæˆå½©è‰²ä¸­æ–‡ç»Ÿè®¡æ‘˜è¦"""
        if not results:
            return "ğŸ“Š æ— æ£€æµ‹ç»“æœ"
        
        total = len(results)
        
        # ç»Ÿè®¡å„é£é™©ç­‰çº§
        risk_counts = {}
        for result in results:
            risk_level = result.get('é£é™©ç­‰çº§', 'æœªçŸ¥')
            risk_counts[risk_level] = risk_counts.get(risk_level, 0) + 1
        
        summary_parts = []
        summary_parts.append("\n" + "=" * 60)
        summary_parts.append("ğŸ“Š æ£€æµ‹ç»Ÿè®¡æ±‡æ€»".center(60))
        summary_parts.append("=" * 60)
        
        # é£é™©ç­‰çº§ç»Ÿè®¡ï¼ˆå¸¦emojiå’Œé¢œè‰²ï¼‰
        risk_emojis = {
            'é«˜é£é™©': ('ğŸš¨', 'red'),
            'ä¸­é£é™©': ('âš ï¸', 'yellow'),
            'ä½é£é™©': ('âœ…', 'green'),
            'æ£€æµ‹å¤±è´¥': ('âŒ', 'magenta')
        }
        
        for risk_level, count in sorted(risk_counts.items()):
            emoji, color = risk_emojis.get(risk_level, ('â€¢', 'white'))
            percentage = (count / total) * 100
            summary_parts.append(f"{emoji} {risk_level}: {count} ä¸ª ({percentage:.1f}%)")
        
        summary_parts.append("-" * 60)
        
        # å®‰å…¨å»ºè®®å’Œæ€»ç»“
        high_risk = risk_counts.get('é«˜é£é™©', 0)
        medium_risk = risk_counts.get('ä¸­é£é™©', 0)
        
        if high_risk > 0:
            summary_parts.append("ğŸš¨ ç«‹å³å¤„ç†: å‘ç°é«˜é£é™©ç½‘ç«™ï¼Œè¯·ç«‹å³å¤„ç†ï¼")
        if medium_risk > 0:
            summary_parts.append("âš ï¸ è°¨æ…è®¿é—®: å‘ç°ä¸­é£é™©ç½‘ç«™ï¼Œå»ºè®®è¿›ä¸€æ­¥éªŒè¯")
        if high_risk == 0 and medium_risk == 0:
            summary_parts.append("âœ… å®‰å…¨è‰¯å¥½: æœ¬æ¬¡æ£€æµ‹æœªå‘ç°æ˜æ˜¾é£é™©ç½‘ç«™")
            
        summary_parts.append("\nğŸ“‹ å»ºè®®æ“ä½œ:")
        summary_parts.append("1. é«˜é£é™©ç½‘ç«™ï¼šé¿å…è®¿é—®ï¼Œç«‹å³åŠ å…¥é»‘åå•")
        summary_parts.append("2. ä¸­é£é™©ç½‘ç«™ï¼šè°¨æ…è®¿é—®ï¼ŒéªŒè¯çœŸå®æ€§")
        summary_parts.append("3. ä½é£é™©ç½‘ç«™ï¼šå¯æ­£å¸¸è®¿é—®ï¼Œä½†ä¿æŒè­¦æƒ•")
        
        return '\n'.join(summary_parts)


    def save_results(self, output_prefix=None):
        """ä¿å­˜æ£€æµ‹ç»“æœ"""
        if not output_prefix:
            output_prefix = f"detection_results_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # ä¿å­˜JSONæ ¼å¼
        json_file = f"{output_prefix}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜CSVæ ¼å¼
        csv_file = f"{output_prefix}.csv"
        if self.results:
            import csv
            with open(csv_file, 'w', encoding='utf-8-sig', newline='') as f:
                writer = csv.writer(f)
                # å†™å…¥è¡¨å¤´
                headers = ['ç½‘å€', 'é£é™©ç­‰çº§', 'é£é™©è¯„åˆ†', 'æ£€æµ‹æ—¶é—´']
                writer.writerow(headers)
                
                # å†™å…¥æ•°æ®
                for result in self.results:
                    row = [
                        result.get('ç½‘å€', ''),
                        result.get('é£é™©ç­‰çº§', ''),
                        result.get('é£é™©è¯„åˆ†', ''),
                        result.get('æ£€æµ‹æ—¶é—´', '')
                    ]
                    writer.writerow(row)
        # æ–°å¢ï¼šä¿å­˜åˆ°æ•°æ®åº“
        try:
            logger.info("æ­£åœ¨ä¿å­˜ç»“æœåˆ°æ•°æ®åº“...")
            save_results_to_database(self.results)
            logger.info("ç»“æœä¿å­˜åˆ°æ•°æ®åº“æˆåŠŸ")
        except Exception as e:
            logger.error(f"ä¿å­˜ç»“æœåˆ°æ•°æ®åº“æ—¶å‡ºé”™: {e}")
        
        return json_file, csv_file
    
    def generate_report(self):
        """ç”Ÿæˆä¸­æ–‡æ£€æµ‹æŠ¥å‘Š"""
        if not self.results:
            return "æ— æ£€æµ‹ç»“æœ"
        
        report_lines = []
        
        # æŠ¥å‘Šæ ‡é¢˜
        report_lines.append("=" * 60)
        report_lines.append("ğŸ›¡ï¸ è¿æ³•ç½‘ç«™æ£€æµ‹æŠ¥å‘Š".center(60))
        report_lines.append("=" * 60)
        report_lines.append(f"æ£€æµ‹æ—¶é—´: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append(f"æ£€æµ‹ç½‘ç«™æ€»æ•°: {len(self.results)} ä¸ª")
        report_lines.append("")
        
        # é£é™©ç­‰çº§ç»Ÿè®¡
        risk_counts = {}
        for result in self.results:
            risk_level = result.get('é£é™©ç­‰çº§', 'æœªçŸ¥')
            risk_counts[risk_level] = risk_counts.get(risk_level, 0) + 1
        
        report_lines.append("ğŸ“Š é£é™©ç­‰çº§ç»Ÿè®¡:")
        for risk_level, count in sorted(risk_counts.items()):
            percentage = (count / len(self.results)) * 100
            report_lines.append(f"  {risk_level}: {count} ä¸ª ({percentage:.1f}%)")
        report_lines.append("")
        
        # é«˜é£é™©ç½‘ç«™è¯¦ç»†åˆ†æ
        high_risk_sites = [r for r in self.results if r.get('é£é™©ç­‰çº§') == 'é«˜é£é™©']
        if high_risk_sites:
            report_lines.append("ğŸš¨ é«˜é£é™©ç½‘ç«™è¯¦æƒ…:")
            for site in high_risk_sites:
                report_lines.append(f"  â€¢ {site.get('ç½‘å€', '')}")
                report_lines.append(f"    é£é™©è¯„åˆ†: {site.get('é£é™©è¯„åˆ†', '')}")
                report_lines.append(f"    é£é™©æè¿°: {site.get('é£é™©æè¿°', '').split(chr(10))[0]}")
                report_lines.append("")
        
        # ä¸­é£é™©ç½‘ç«™åˆ—è¡¨
        medium_risk_sites = [r for r in self.results if r.get('é£é™©ç­‰çº§') == 'ä¸­é£é™©']
        if medium_risk_sites:
            report_lines.append("âš ï¸ ä¸­é£é™©ç½‘ç«™åˆ—è¡¨:")
            for site in medium_risk_sites:
                report_lines.append(f"  â€¢ {site.get('ç½‘å€', '')}")
            report_lines.append("")
        
        # æ£€æµ‹å¤±è´¥ç½‘ç«™
        failed_sites = [r for r in self.results if r.get('é£é™©ç­‰çº§') == 'æ£€æµ‹å¤±è´¥']
        if failed_sites:
            report_lines.append("âŒ æ£€æµ‹å¤±è´¥ç½‘ç«™:")
            for site in failed_sites:
                report_lines.append(f"  â€¢ {site.get('ç½‘å€', '')}")
                if 'é”™è¯¯ä¿¡æ¯' in site:
                    report_lines.append(f"    é”™è¯¯: {site['é”™è¯¯ä¿¡æ¯']}")
            report_lines.append("")
        
        # å®‰å…¨å»ºè®®
        report_lines.append("ğŸ’¡ å®‰å…¨å»ºè®®ä¸å¤„ç†æ–¹æ¡ˆ:")
        if high_risk_sites:
            report_lines.append("  1. é«˜é£é™©ç½‘ç«™: ç«‹å³é¿å…è®¿é—®ï¼ŒåŠ å…¥é»‘åå•")
            report_lines.append("  2. é€šçŸ¥ç›¸å…³äººå‘˜: å°†é«˜é£é™©ç½‘ç«™ä¿¡æ¯åˆ†äº«ç»™å›¢é˜Ÿ")
            report_lines.append("  3. æŒç»­ç›‘æ§: å®šæœŸæ£€æŸ¥è¿™äº›ç½‘ç«™çš„çŠ¶æ€")
        
        if medium_risk_sites:
            report_lines.append("  4. ä¸­é£é™©ç½‘ç«™: è°¨æ…è®¿é—®ï¼Œå»ºè®®äººå·¥éªŒè¯")
            report_lines.append("  5. äºŒæ¬¡æ£€æµ‹: 24å°æ—¶åé‡æ–°æ£€æµ‹ä¸­é£é™©ç½‘ç«™")
        
        report_lines.append("  6. é¢„é˜²æªæ–½: åŠ å¼ºå‘˜å·¥å®‰å…¨æ„è¯†åŸ¹è®­")
        report_lines.append("  7. å®šæœŸæ£€æµ‹: å»ºè®®æ¯å‘¨è¿›è¡Œä¸€æ¬¡æ‰¹é‡æ£€æµ‹")
        report_lines.append("")
        
        # åç»­è¡ŒåŠ¨è®¡åˆ’
        report_lines.append("ğŸ“… åç»­è¡ŒåŠ¨è®¡åˆ’:")
        report_lines.append("  â€¢ ç«‹å³: å¤„ç†æ‰€æœ‰é«˜é£é™©ç½‘ç«™")
        report_lines.append("  â€¢ 24å°æ—¶å†…: äººå·¥éªŒè¯ä¸­é£é™©ç½‘ç«™")
        report_lines.append("  â€¢ æœ¬å‘¨å†…: æ›´æ–°é»‘åå•æ•°æ®åº“")
        report_lines.append("  â€¢ ä¸‹å‘¨: å®‰æ’æ–°ä¸€è½®æ£€æµ‹")
        
        return "\n".join(report_lines)

    def print_summary(self, results):
        """æ‰“å°ç»Ÿè®¡æ‘˜è¦"""
        if not results:
            return
            
        total = len(results)
        risk_counts = {}
        for result in results:
            risk_level = result.get('é£é™©ç­‰çº§', 'æœªçŸ¥')
            risk_counts[risk_level] = risk_counts.get(risk_level, 0) + 1
        
        print("\n" + "=" * 60)
        print("ğŸ“Š æ£€æµ‹ç»Ÿè®¡æ±‡æ€»".center(60))
        print("=" * 60)
        
        risk_emojis = {
            'é«˜é£é™©': 'ğŸš¨',
            'ä¸­é£é™©': 'âš ï¸',
            'ä½é£é™©': 'âœ…',
            'æ£€æµ‹å¤±è´¥': 'âŒ'
        }
        
        for risk_level, count in sorted(risk_counts.items()):
            emoji = risk_emojis.get(risk_level, 'â€¢')
            percentage = (count / total) * 100
            print(f"{emoji} {risk_level}: {count} ä¸ª ({percentage:.1f}%)")
        
        print("-" * 60)
        
        high_risk = risk_counts.get('é«˜é£é™©', 0)
        medium_risk = risk_counts.get('ä¸­é£é™©', 0)
        
        if high_risk > 0:
            print("ğŸš¨ ç«‹å³å¤„ç†: å‘ç°é«˜é£é™©ç½‘ç«™ï¼Œè¯·ç«‹å³å¤„ç†ï¼")
        if medium_risk > 0:
            print("âš ï¸ è°¨æ…è®¿é—®: å‘ç°ä¸­é£é™©ç½‘ç«™ï¼Œå»ºè®®è¿›ä¸€æ­¥éªŒè¯")
        if high_risk == 0 and medium_risk == 0:
            print("âœ… å®‰å…¨è‰¯å¥½: æœ¬æ¬¡æ£€æµ‹æœªå‘ç°æ˜æ˜¾é£é™©ç½‘ç«™")

# æ·»åŠ å‡½æ•°ä»MySQLæ•°æ®åº“æŸ¥è¯¢URL
def get_urls_from_mysql():
    """ä»MySQLæ•°æ®åº“æŸ¥è¯¢URLåˆ—è¡¨å¹¶è¿›è¡Œå»é‡ï¼Œç„¶åå†™å…¥sample_urls.txt"""
    urls = []
    try:
        # è¿æ¥æ•°æ®åº“
        connection = pymysql.connect(**DB_CONFIG)
        with connection.cursor() as cursor:
            # æ‰§è¡ŒSQLæŸ¥è¯¢
            sql = "select url from gat_illegal_result where  discovery_method not in (4,5)   and url not in (select url from gat_illegal_result_detector)   order by update_time desc   LIMIT 5"
            # sql = "select url from gat_illegal_result where  discovery_method not in (4,5)  LIMIT 5"
            cursor.execute(sql)
            # è·å–æ‰€æœ‰æŸ¥è¯¢ç»“æœ
            results = cursor.fetchall()
            # æå–URLs
            urls = [row['url'] for row in results]
        
        # å¯¹URLè¿›è¡Œå»é‡
        unique_urls = list(set(urls))
        
        # å°†å»é‡åçš„URLå†™å…¥sample_urls.txtæ–‡ä»¶
        sample_urls_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'sample_urls.txt')
        with open(sample_urls_path, 'w', encoding='utf-8') as f:
            for url in unique_urls:
                f.write(f"{url}\n")
        
        color_printer = ColorPrinter()
        color_printer.print_success(f"æˆåŠŸä»æ•°æ®åº“æŸ¥è¯¢åˆ° {len(urls)} ä¸ªURLï¼Œå»é‡åå‰©ä½™ {len(unique_urls)} ä¸ª")
        color_printer.print_success(f"å·²å°†å»é‡åçš„URLå†™å…¥ {sample_urls_path}")
        
        return unique_urls
    except Exception as e:
        color_printer = ColorPrinter()
        color_printer.print_error(f"ä»æ•°æ®åº“æŸ¥è¯¢URLæˆ–å†™å…¥æ–‡ä»¶å¤±è´¥: {e}")
    finally:
        if 'connection' in locals() and connection.open:
            connection.close()
    return urls
# åˆ›å»ºæ£€æµ‹ç»“æœè¡¨
def create_detector_result_table():
    """åˆ›å»ºæ£€æµ‹ç»“æœè¡¨"""
    try:
        # è¿æ¥æ•°æ®åº“
        connection = pymysql.connect(**DB_CONFIG)
        try:
            with connection.cursor() as cursor:
                # åˆ›å»ºè¡¨çš„SQLè¯­å¥ï¼Œæ·»åŠ äº†è¡¨æ³¨é‡Šå’Œå­—æ®µæ³¨é‡Š
                create_table_sql = """
                CREATE TABLE IF NOT EXISTS gat_illegal_result_detector (
                    id INT AUTO_INCREMENT PRIMARY KEY COMMENT 'ä¸»é”®ID',
                    url VARCHAR(255) NOT NULL UNIQUE COMMENT 'æ£€æµ‹çš„ç½‘å€',
                    risk_level VARCHAR(20) NOT NULL COMMENT 'é£é™©ç­‰çº§(ä½é£é™©/ä¸­é£é™©/é«˜é£é™©)',
                    risk_score INT NOT NULL COMMENT 'é£é™©è¯„åˆ†(0-100)',
                    risk_description TEXT COMMENT 'é£é™©æè¿°ä¿¡æ¯',
                    detection_time DATETIME NOT NULL COMMENT 'æ£€æµ‹æ—¶é—´',
                    domain_length INT COMMENT 'åŸŸåé•¿åº¦',
                    subdomain_count INT COMMENT 'å­åŸŸåæ•°é‡',
                    has_hyphen TINYINT(1) COMMENT 'æ˜¯å¦åŒ…å«è¿å­—ç¬¦',
                    has_digits TINYINT(1) COMMENT 'æ˜¯å¦åŒ…å«æ•°å­—',
                    suspicious_tld TINYINT(1) COMMENT 'æ˜¯å¦ä¸ºå¯ç–‘é¡¶çº§åŸŸå',
                    digit_ratio FLOAT COMMENT 'æ•°å­—æ¯”ä¾‹',
                    special_char_ratio FLOAT COMMENT 'ç‰¹æ®Šå­—ç¬¦æ¯”ä¾‹',
                    consonant_ratio FLOAT COMMENT 'è¾…éŸ³æ¯”ä¾‹',
                    entropy FLOAT COMMENT 'ç†µå€¼(éšæœºæ€§)',
                    in_blacklist TINYINT(1) COMMENT 'æ˜¯å¦åœ¨é»‘åå•ä¸­',
                    brand_similarity FLOAT COMMENT 'å“ç‰Œç›¸ä¼¼åº¦',
                    potential_phishing TINYINT(1) COMMENT 'æ˜¯å¦ç–‘ä¼¼é’“é±¼',
                    homograph_attack TINYINT(1) COMMENT 'æ˜¯å¦å­˜åœ¨åŒå½¢å¼‚ä¹‰æ”»å‡»',
                    suspicious_combo INT COMMENT 'å¯ç–‘å…³é”®è¯ç»„åˆæ•°',
                    domain_age_days INT COMMENT 'åŸŸåå¹´é¾„(å¤©)',
                    is_new_domain TINYINT(1) COMMENT 'æ˜¯å¦ä¸ºæ–°åŸŸå(30å¤©å†…)',
                    is_very_new_domain TINYINT(1) COMMENT 'æ˜¯å¦ä¸ºææ–°åŸŸå(7å¤©å†…)',
                    days_to_expire INT COMMENT 'åˆ°æœŸå‰©ä½™å¤©æ•°',
                    short_registration TINYINT(1) COMMENT 'æ˜¯å¦ä¸ºçŸ­æœŸæ³¨å†Œ(å°‘äº1å¹´)',
                    suspicious_registrar TINYINT(1) COMMENT 'æ˜¯å¦ä¸ºå¯ç–‘æ³¨å†Œå•†',
                    content_length INT COMMENT 'å†…å®¹é•¿åº¦',
                    text_length INT COMMENT 'æ–‡æœ¬é•¿åº¦',
                    image_count INT COMMENT 'å›¾ç‰‡æ•°é‡',
                    link_count INT COMMENT 'é“¾æ¥æ•°é‡',
                    form_count INT COMMENT 'è¡¨å•æ•°é‡',
                    external_links INT COMMENT 'å¤–éƒ¨é“¾æ¥æ•°',
                    sensitive_keyword_count INT COMMENT 'æ•æ„Ÿè¯æ€»æ•°',
                    sensitive_keyword_ratio FLOAT COMMENT 'æ•æ„Ÿè¯å æ¯”',
                    has_title TINYINT(1) COMMENT 'æ˜¯å¦æœ‰æ ‡é¢˜',
                    title_length INT COMMENT 'æ ‡é¢˜é•¿åº¦',
                    has_description TINYINT(1) COMMENT 'æ˜¯å¦æœ‰æè¿°',
                    has_keywords TINYINT(1) COMMENT 'æ˜¯å¦æœ‰å…³é”®è¯',
                    has_robots TINYINT(1) COMMENT 'æ˜¯å¦æœ‰robots.txt',
                    has_login_form TINYINT(1) COMMENT 'æ˜¯å¦æœ‰ç™»å½•è¡¨å•',
                    has_contact_info TINYINT(1) COMMENT 'æ˜¯å¦æœ‰è”ç³»ä¿¡æ¯',
                    has_privacy_policy TINYINT(1) COMMENT 'æ˜¯å¦æœ‰éšç§æ”¿ç­–',
                    suspicious_images TINYINT(1) COMMENT 'æ˜¯å¦æœ‰å¯ç–‘å›¾ç‰‡',
                    script_count INT COMMENT 'è„šæœ¬æ•°é‡',
                    suspicious_scripts TINYINT(1) COMMENT 'æ˜¯å¦æœ‰å¯ç–‘è„šæœ¬',
                    redirect_count INT COMMENT 'é‡å®šå‘æ¬¡æ•°',
                    final_url VARCHAR(255) COMMENT 'æœ€ç»ˆé‡å®šå‘åçš„ç½‘å€',
                    domain_changed TINYINT(1) COMMENT 'æ˜¯å¦å‘ç”ŸåŸŸåå˜æ›´',
                    has_ssl TINYINT(1) COMMENT 'æ˜¯å¦æœ‰SSLè¯ä¹¦',
                    ssl_valid TINYINT(1) COMMENT 'SSLè¯ä¹¦æ˜¯å¦æœ‰æ•ˆ',
                    trusted_ca TINYINT(1) COMMENT 'æ˜¯å¦ä¸ºå¯ä¿¡CAé¢å‘',
                    cert_valid_days INT COMMENT 'è¯ä¹¦æœ‰æ•ˆå¤©æ•°',
                    cert_too_new TINYINT(1) COMMENT 'è¯ä¹¦æ˜¯å¦å¤ªæ–°',
                    ssl_domain_match TINYINT(1) COMMENT 'åŸŸåæ˜¯å¦åŒ¹é…',
                    wildcard_cert TINYINT(1) COMMENT 'æ˜¯å¦ä¸ºé€šé…ç¬¦è¯ä¹¦',
                    dns_resolved TINYINT(1) COMMENT 'DNSæ˜¯å¦è§£ææˆåŠŸ',
                    ip_count INT COMMENT 'IPæ•°é‡',
                    first_ip VARCHAR(50) COMMENT 'é¦–ä¸ªIPåœ°å€',
                    blacklisted_ip TINYINT(1) COMMENT 'IPæ˜¯å¦åœ¨é»‘åå•ä¸­',
                    has_mx TINYINT(1) COMMENT 'æ˜¯å¦æœ‰MXè®°å½•',
                    mx_count INT COMMENT 'MXè®°å½•æ•°é‡',
                    has_spf TINYINT(1) COMMENT 'æ˜¯å¦æœ‰SPFè®°å½•',
                    response_time FLOAT COMMENT 'å“åº”æ—¶é—´(ç§’)',
                    http_status INT COMMENT 'HTTPçŠ¶æ€ç ',
                    web_accessible TINYINT(1) COMMENT 'ç½‘ç«™æ˜¯å¦å¯è®¿é—®',
                    server_header VARCHAR(100) COMMENT 'æœåŠ¡å™¨ä¿¡æ¯',
                    hsts TINYINT(1) COMMENT 'æ˜¯å¦å¯ç”¨HSTSå®‰å…¨å¤´',
                    x_frame_options TINYINT(1) COMMENT 'æ˜¯å¦è®¾ç½®X-Frame-Options',
                    x_content_type TINYINT(1) COMMENT 'æ˜¯å¦è®¾ç½®X-Content-Type-Options',
                    x_xss_protection TINYINT(1) COMMENT 'æ˜¯å¦è®¾ç½®X-XSS-Protection',
                    csp TINYINT(1) COMMENT 'æ˜¯å¦è®¾ç½®Content-Security-Policy',
                    sensitive_è¿è§„ä¹¦ç± INT COMMENT 'è¿è§„ä¹¦ç±å…³é”®è¯æ•°é‡',
                    sensitive_ç½‘ç«™è¿ç¦è¯ INT COMMENT 'ç½‘ç«™è¿ç¦è¯æ•°é‡',
                    sensitive_æ¶‰ç¨³ INT COMMENT 'æ¶‰ç¨³å…³é”®è¯æ•°é‡',
                    sensitive_æ¶‰é»„ INT COMMENT 'æ¶‰é»„å…³é”®è¯æ•°é‡',
                    sensitive_æ¶‰èµŒ INT COMMENT 'æ¶‰èµŒå…³é”®è¯æ•°é‡',
                    sensitive_æ¶‰æ”¿ INT COMMENT 'æ¶‰æ”¿å…³é”®è¯æ•°é‡',
                    sensitive_æ¶‰æªæš´ INT COMMENT 'æ¶‰æªæš´å…³é”®è¯æ•°é‡',
                    sensitive_æ¶‰ææ¶‰é‚ª INT COMMENT 'æ¶‰ææ¶‰é‚ªå…³é”®è¯æ•°é‡',
                    sensitive_æ¶‰é»‘ç°äº§ INT COMMENT 'æ¶‰é»‘ç°äº§å…³é”®è¯æ•°é‡',
                    sensitive_æ¶‰ç”µè¯ˆ INT COMMENT 'æ¶‰ç”µè¯ˆå…³é”®è¯æ•°é‡',
                    sensitive_è¿è§„åŒ–å­¦å“ INT COMMENT 'è¿è§„åŒ–å­¦å“å…³é”®è¯æ•°é‡',
                    subpage_count INT COMMENT 'æ£€æµ‹å­é¡µé¢æ•°é‡',
                    suspicious_subpages INT COMMENT 'å¯ç–‘å­é¡µé¢æ•°',
                    avg_subpage_risk FLOAT COMMENT 'å­é¡µé¢å¹³å‡é£é™©',
                    has_sensitive_subpage TINYINT(1) COMMENT 'æ˜¯å¦åŒ…å«æ•æ„Ÿå­é¡µé¢',
                    subpage_keywords TEXT COMMENT 'å­é¡µé¢å…³é”®è¯ç»Ÿè®¡(JSONæ ¼å¼)',
                    subpage_details TEXT COMMENT 'å­é¡µé¢è¯¦ç»†ä¿¡æ¯(JSONæ ¼å¼)',
                    create_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP COMMENT 'åˆ›å»ºæ—¶é—´',
                    update_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT 'æ›´æ–°æ—¶é—´'
                ) COMMENT='è¿æ³•ç½‘ç«™æ£€æµ‹ç»“æœè¡¨' 
                """
                cursor.execute(create_table_sql)
                connection.commit()
                logger.info("æ£€æµ‹ç»“æœè¡¨åˆ›å»ºæˆåŠŸ")
        finally:
            connection.close()
    except Exception as e:
        logger.error(f"åˆ›å»ºæ£€æµ‹ç»“æœè¡¨å¤±è´¥: {e}")


def save_result_to_database(result):
    """ä¿å­˜æ£€æµ‹ç»“æœåˆ°æ•°æ®åº“ï¼Œå¦‚æœå­˜åœ¨åˆ™æ›´æ–°ï¼Œä¸å­˜åœ¨åˆ™æ–°å¢"""
    try:
        # è¿æ¥æ•°æ®åº“
        connection = pymysql.connect(**DB_CONFIG)
        try:
            with connection.cursor() as cursor:
                # æ£€æŸ¥è®°å½•æ˜¯å¦å­˜åœ¨
                check_sql = "SELECT id FROM gat_illegal_result_detector WHERE url = %s"
                cursor.execute(check_sql, (result['ç½‘å€'],))
                exists = cursor.fetchone() is not None
                
                # è·å–è¯¦ç»†ç‰¹å¾
                features = result['è¯¦ç»†ç‰¹å¾']
                en_features = result['è‹±æ–‡åŸæ–‡']['features']
                # å°†å­é¡µé¢ç‰¹å¾è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²
            #     'subpage_keywords': 'å­é¡µé¢ä¸­å‘ç°çš„å…³é”®è¯ç»Ÿè®¡',
            # 'subpage_details': 'å­é¡µé¢è¯¦ç»†ä¿¡æ¯'
                subpage_keywords = json.dumps(features.get('å­é¡µé¢ä¸­å‘ç°çš„å…³é”®è¯ç»Ÿè®¡', {}), ensure_ascii=False) if 'å­é¡µé¢ä¸­å‘ç°çš„å…³é”®è¯ç»Ÿè®¡' in features else None
                subpage_details = json.dumps(features.get('å­é¡µé¢è¯¦ç»†ä¿¡æ¯', []), ensure_ascii=False) if 'å­é¡µé¢è¯¦ç»†ä¿¡æ¯' in features else None
                if exists:
                    # æ›´æ–°è®°å½•
                    update_sql = """
                    UPDATE gat_illegal_result_detector
                    SET 
                        risk_level = %s,
                        risk_score = %s,
                        risk_description = %s,
                        detection_time = %s,
                        domain_length = %s,
                        subdomain_count = %s,
                        has_hyphen = %s,
                        has_digits = %s,
                        suspicious_tld = %s,
                        digit_ratio = %s,
                        special_char_ratio = %s,
                        consonant_ratio = %s,
                        entropy = %s,
                        in_blacklist = %s,
                        brand_similarity = %s,
                        potential_phishing = %s,
                        homograph_attack = %s,
                        suspicious_combo = %s,
                        domain_age_days = %s,
                        is_new_domain = %s,
                        is_very_new_domain = %s,
                        days_to_expire = %s,
                        short_registration = %s,
                        suspicious_registrar = %s,
                        content_length = %s,
                        text_length = %s,
                        image_count = %s,
                        link_count = %s,
                        form_count = %s,
                        external_links = %s,
                        sensitive_keyword_count = %s,
                        sensitive_keyword_ratio = %s,
                        has_title = %s,
                        title_length = %s,
                        has_description = %s,
                        has_keywords = %s,
                        has_robots = %s,
                        has_login_form = %s,
                        has_contact_info = %s,
                        has_privacy_policy = %s,
                        suspicious_images = %s,
                        script_count = %s,
                        suspicious_scripts = %s,
                        redirect_count = %s,
                        final_url = %s,
                        domain_changed = %s,
                        has_ssl = %s,
                        ssl_valid = %s,
                        trusted_ca = %s,
                        cert_valid_days = %s,
                        cert_too_new = %s,
                        ssl_domain_match = %s,
                        wildcard_cert = %s,
                        dns_resolved = %s,
                        ip_count = %s,
                        first_ip = %s,
                        blacklisted_ip = %s,
                        has_mx = %s,
                        mx_count = %s,
                        has_spf = %s,
                        response_time = %s,
                        http_status = %s,
                        web_accessible = %s,
                        server_header = %s,
                        hsts = %s,
                        x_frame_options = %s,
                        x_content_type = %s,
                        x_xss_protection = %s,
                        csp = %s,
                        sensitive_è¿è§„ä¹¦ç± = %s,
                        sensitive_ç½‘ç«™è¿ç¦è¯ = %s,
                        sensitive_æ¶‰ç¨³ = %s,
                        sensitive_æ¶‰é»„ = %s,
                        sensitive_æ¶‰èµŒ = %s,
                        sensitive_æ¶‰æ”¿ = %s,
                        sensitive_æ¶‰æªæš´ = %s,
                        sensitive_æ¶‰ææ¶‰é‚ª = %s,
                        sensitive_æ¶‰é»‘ç°äº§ = %s,
                        sensitive_æ¶‰ç”µè¯ˆ = %s,
                        sensitive_è¿è§„åŒ–å­¦å“ = %s,
                        subpage_count = %s,
                        suspicious_subpages = %s,
                        avg_subpage_risk = %s,
                        has_sensitive_subpage = %s,
                        subpage_keywords = %s,
                        subpage_details = %s

                    WHERE url = %s
                    """
                    
                    # å‡†å¤‡å‚æ•°
                    params = (
                        result['é£é™©ç­‰çº§'],
                        int(result['é£é™©è¯„åˆ†'].replace('%', '')),
                        result['é£é™©æè¿°'],
                        result['æ£€æµ‹æ—¶é—´'],
                        features.get('åŸŸåé•¿åº¦'),
                        features.get('å­åŸŸåæ•°é‡'),
                        features.get('åŒ…å«è¿å­—ç¬¦'),
                        features.get('åŒ…å«æ•°å­—'),
                        features.get('å¯ç–‘é¡¶çº§åŸŸå'),
                        features.get('æ•°å­—æ¯”ä¾‹'),
                        features.get('ç‰¹æ®Šå­—ç¬¦æ¯”ä¾‹'),
                        features.get('è¾…éŸ³æ¯”ä¾‹'),
                        features.get('ç†µå€¼ï¼ˆéšæœºæ€§ï¼‰'),
                        features.get('é»‘åå•åŒ¹é…'),
                        features.get('å“ç‰Œç›¸ä¼¼åº¦'),
                        features.get('ç–‘ä¼¼é’“é±¼'),
                        features.get('åŒå½¢å¼‚ä¹‰æ”»å‡»'),
                        features.get('å¯ç–‘å…³é”®è¯ç»„åˆ'),
                        features.get('åŸŸåå¹´é¾„ï¼ˆå¤©ï¼‰'),
                        features.get('æ–°åŸŸåï¼ˆ30å¤©å†…ï¼‰'),
                        features.get('ææ–°åŸŸåï¼ˆ7å¤©å†…ï¼‰'),
                        features.get('åˆ°æœŸå‰©ä½™å¤©æ•°'),
                        features.get('çŸ­æœŸæ³¨å†Œ'),
                        features.get('å¯ç–‘æ³¨å†Œå•†'),
                        features.get('å†…å®¹é•¿åº¦'),
                        features.get('æ–‡æœ¬é•¿åº¦'),
                        features.get('å›¾ç‰‡æ•°é‡'),
                        features.get('é“¾æ¥æ•°é‡'),
                        features.get('è¡¨å•æ•°é‡'),
                        features.get('å¤–éƒ¨é“¾æ¥æ•°'),
                        features.get('æ•æ„Ÿè¯æ€»æ•°'),
                        features.get('æ•æ„Ÿè¯å æ¯”'),
                        features.get('æœ‰æ ‡é¢˜'),
                        features.get('æ ‡é¢˜é•¿åº¦'),
                        features.get('æœ‰æè¿°'),
                        features.get('æœ‰å…³é”®è¯'),
                        features.get('æœ‰robots'),
                        features.get('æœ‰ç™»å½•è¡¨å•'),
                        features.get('æœ‰è”ç³»ä¿¡æ¯'),
                        features.get('æœ‰éšç§æ”¿ç­–'),
                        features.get('å¯ç–‘å›¾ç‰‡'),
                        features.get('è„šæœ¬æ•°é‡'),
                        features.get('å¯ç–‘è„šæœ¬'),
                        features.get('é‡å®šå‘æ¬¡æ•°'),
                        features.get('æœ€ç»ˆç½‘å€'),
                        features.get('åŸŸåå˜æ›´'),
                        features.get('æœ‰SSLè¯ä¹¦'),
                        features.get('SSLæœ‰æ•ˆ'),
                        features.get('å¯ä¿¡CA'),
                        features.get('è¯ä¹¦æœ‰æ•ˆå¤©æ•°'),
                        features.get('è¯ä¹¦å¤ªæ–°'),
                        features.get('åŸŸååŒ¹é…'),
                        features.get('é€šé…ç¬¦è¯ä¹¦'),
                        features.get('DNSè§£ææˆåŠŸ'),
                        features.get('IPæ•°é‡'),
                        features.get('é¦–ä¸ªIP'),
                        features.get('IPé»‘åå•'),
                        features.get('æœ‰MXè®°å½•'),
                        features.get('MXè®°å½•æ•°'),
                        features.get('æœ‰SPFè®°å½•'),
                        features.get('å“åº”æ—¶é—´'),
                        features.get('HTTPçŠ¶æ€ç '),
                        features.get('å¯è®¿é—®'),
                        features.get('æœåŠ¡å™¨ä¿¡æ¯'),
                        features.get('HSTSå®‰å…¨å¤´'),
                        features.get('X-Frame-Options'),
                        features.get('X-Content-Type-Options'),
                        features.get('X-XSS-Protection'),
                        features.get('Content-Security-Policy'),
                        en_features.get('sensitive_è¿è§„ä¹¦ç±'),
                        en_features.get('sensitive_ç½‘ç«™è¿ç¦è¯'),
                        en_features.get('sensitive_æ¶‰ç¨³'),
                        en_features.get('sensitive_æ¶‰é»„'),
                        en_features.get('sensitive_æ¶‰èµŒ'),
                        en_features.get('sensitive_æ¶‰æ”¿'),
                        en_features.get('sensitive_æ¶‰æªæš´'),
                        en_features.get('sensitive_æ¶‰ææ¶‰é‚ª'),
                        en_features.get('sensitive_æ¶‰é»‘ç°äº§'),
                        en_features.get('sensitive_æ¶‰ç”µè¯ˆ'),
                        en_features.get('sensitive_è¿è§„åŒ–å­¦å“'),
                        features.get('æ£€æµ‹å­é¡µé¢æ•°é‡'),
                        features.get('å¯ç–‘å­é¡µé¢æ•°'),
                        features.get('å­é¡µé¢å¹³å‡é£é™©'),
                        features.get('åŒ…å«æ•æ„Ÿå­é¡µé¢'),
                        subpage_keywords,
                        subpage_details,
                        result['ç½‘å€']
                    )
                    
                    cursor.execute(update_sql, params)
                    logger.info(f"æ›´æ–°æ£€æµ‹ç»“æœæˆåŠŸ: {result['ç½‘å€']}")
                else:
                    # æ’å…¥æ–°è®°å½•
                    insert_sql = """
                    INSERT INTO gat_illegal_result_detector (
                        url,
                        risk_level,
                        risk_score,
                        risk_description,
                        detection_time,
                        domain_length,
                        subdomain_count,
                        has_hyphen,
                        has_digits,
                        suspicious_tld,
                        digit_ratio,
                        special_char_ratio,
                        consonant_ratio,
                        entropy,
                        in_blacklist,
                        brand_similarity,
                        potential_phishing,
                        homograph_attack,
                        suspicious_combo,
                        domain_age_days,
                        is_new_domain,
                        is_very_new_domain,
                        days_to_expire,
                        short_registration,
                        suspicious_registrar,
                        content_length,
                        text_length,
                        image_count,
                        link_count,
                        form_count,
                        external_links,
                        sensitive_keyword_count,
                        sensitive_keyword_ratio,
                        has_title,
                        title_length,
                        has_description,
                        has_keywords,
                        has_robots,
                        has_login_form,
                        has_contact_info,
                        has_privacy_policy,
                        suspicious_images,
                        script_count,
                        suspicious_scripts,
                        redirect_count,
                        final_url,
                        domain_changed,
                        has_ssl,
                        ssl_valid,
                        trusted_ca,
                        cert_valid_days,
                        cert_too_new,
                        ssl_domain_match,
                        wildcard_cert,
                        dns_resolved,
                        ip_count,
                        first_ip,
                        blacklisted_ip,
                        has_mx,
                        mx_count,
                        has_spf,
                        response_time,
                        http_status,
                        web_accessible,
                        server_header,
                        hsts,
                        x_frame_options,
                        x_content_type,
                        x_xss_protection,
                        csp,
                        sensitive_è¿è§„ä¹¦ç±,
                        sensitive_ç½‘ç«™è¿ç¦è¯,
                        sensitive_æ¶‰ç¨³,
                        sensitive_æ¶‰é»„,
                        sensitive_æ¶‰èµŒ,
                        sensitive_æ¶‰æ”¿,
                        sensitive_æ¶‰æªæš´,
                        sensitive_æ¶‰ææ¶‰é‚ª,
                        sensitive_æ¶‰é»‘ç°äº§,
                        sensitive_æ¶‰ç”µè¯ˆ,
                        sensitive_è¿è§„åŒ–å­¦å“,
                        subpage_count,
                        suspicious_subpages,
                        avg_subpage_risk,
                        has_sensitive_subpage,
                        subpage_keywords,
                        subpage_details
                    )VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                    """
                    
                    # å‡†å¤‡å‚æ•°ï¼ˆä¸æ›´æ–°æ—¶ç›¸åŒï¼‰
                    params = (
                        result['ç½‘å€'],
                        result['é£é™©ç­‰çº§'],
                        int(result['é£é™©è¯„åˆ†'].replace('%', '')),
                        result['é£é™©æè¿°'],
                        result['æ£€æµ‹æ—¶é—´'],
                        features.get('åŸŸåé•¿åº¦'),
                        features.get('å­åŸŸåæ•°é‡'),
                        features.get('åŒ…å«è¿å­—ç¬¦'),
                        features.get('åŒ…å«æ•°å­—'),
                        features.get('å¯ç–‘é¡¶çº§åŸŸå'),
                        features.get('æ•°å­—æ¯”ä¾‹'),
                        features.get('ç‰¹æ®Šå­—ç¬¦æ¯”ä¾‹'),
                        features.get('è¾…éŸ³æ¯”ä¾‹'),
                        features.get('ç†µå€¼ï¼ˆéšæœºæ€§ï¼‰'),
                        features.get('é»‘åå•åŒ¹é…'),
                        features.get('å“ç‰Œç›¸ä¼¼åº¦'),
                        features.get('ç–‘ä¼¼é’“é±¼'),
                        features.get('åŒå½¢å¼‚ä¹‰æ”»å‡»'),
                        features.get('å¯ç–‘å…³é”®è¯ç»„åˆ'),
                        features.get('åŸŸåå¹´é¾„ï¼ˆå¤©ï¼‰'),
                        features.get('æ–°åŸŸåï¼ˆ30å¤©å†…ï¼‰'),
                        features.get('ææ–°åŸŸåï¼ˆ7å¤©å†…ï¼‰'),
                        features.get('åˆ°æœŸå‰©ä½™å¤©æ•°'),
                        features.get('çŸ­æœŸæ³¨å†Œ'),
                        features.get('å¯ç–‘æ³¨å†Œå•†'),
                        features.get('å†…å®¹é•¿åº¦'),
                        features.get('æ–‡æœ¬é•¿åº¦'),
                        features.get('å›¾ç‰‡æ•°é‡'),
                        features.get('é“¾æ¥æ•°é‡'),
                        features.get('è¡¨å•æ•°é‡'),
                        features.get('å¤–éƒ¨é“¾æ¥æ•°'),
                        features.get('æ•æ„Ÿè¯æ€»æ•°'),
                        features.get('æ•æ„Ÿè¯å æ¯”'),
                        features.get('æœ‰æ ‡é¢˜'),
                        features.get('æ ‡é¢˜é•¿åº¦'),
                        features.get('æœ‰æè¿°'),
                        features.get('æœ‰å…³é”®è¯'),
                        features.get('æœ‰robots'),
                        features.get('æœ‰ç™»å½•è¡¨å•'),
                        features.get('æœ‰è”ç³»ä¿¡æ¯'),
                        features.get('æœ‰éšç§æ”¿ç­–'),
                        features.get('å¯ç–‘å›¾ç‰‡'),
                        features.get('è„šæœ¬æ•°é‡'),
                        features.get('å¯ç–‘è„šæœ¬'),
                        features.get('é‡å®šå‘æ¬¡æ•°'),
                        features.get('æœ€ç»ˆç½‘å€'),
                        features.get('åŸŸåå˜æ›´'),
                        features.get('æœ‰SSLè¯ä¹¦'),
                        features.get('SSLæœ‰æ•ˆ'),
                        features.get('å¯ä¿¡CA'),
                        features.get('è¯ä¹¦æœ‰æ•ˆå¤©æ•°'),
                        features.get('è¯ä¹¦å¤ªæ–°'),
                        features.get('åŸŸååŒ¹é…'),
                        features.get('é€šé…ç¬¦è¯ä¹¦'),
                        features.get('DNSè§£ææˆåŠŸ'),
                        features.get('IPæ•°é‡'),
                        features.get('é¦–ä¸ªIP'),
                        features.get('IPé»‘åå•'),
                        features.get('æœ‰MXè®°å½•'),
                        features.get('MXè®°å½•æ•°'),
                        features.get('æœ‰SPFè®°å½•'),
                        features.get('å“åº”æ—¶é—´'),
                        features.get('HTTPçŠ¶æ€ç '),
                        features.get('å¯è®¿é—®'),
                        features.get('æœåŠ¡å™¨ä¿¡æ¯'),
                        features.get('HSTSå®‰å…¨å¤´'),
                        features.get('X-Frame-Options'),
                        features.get('X-Content-Type-Options'),
                        features.get('X-XSS-Protection'),
                        features.get('Content-Security-Policy'),
                        en_features.get('sensitive_è¿è§„ä¹¦ç±'),
                        en_features.get('sensitive_ç½‘ç«™è¿ç¦è¯'),
                        en_features.get('sensitive_æ¶‰ç¨³'),
                        en_features.get('sensitive_æ¶‰é»„'),
                        en_features.get('sensitive_æ¶‰èµŒ'),
                        en_features.get('sensitive_æ¶‰æ”¿'),
                        en_features.get('sensitive_æ¶‰æªæš´'),
                        en_features.get('sensitive_æ¶‰ææ¶‰é‚ª'),
                        en_features.get('sensitive_æ¶‰é»‘ç°äº§'),
                        en_features.get('sensitive_æ¶‰ç”µè¯ˆ'),
                        en_features.get('sensitive_è¿è§„åŒ–å­¦å“'),
                        features.get('æ£€æµ‹å­é¡µé¢æ•°é‡'),
                        features.get('å¯ç–‘å­é¡µé¢æ•°'),
                        features.get('å­é¡µé¢å¹³å‡é£é™©'),
                        features.get('åŒ…å«æ•æ„Ÿå­é¡µé¢'),
                        subpage_keywords,
                        subpage_details
                    )
                    
                    cursor.execute(insert_sql, params)
                    logger.info(f"æ’å…¥æ£€æµ‹ç»“æœæˆåŠŸ: {result['ç½‘å€']}")
                
                connection.commit()
        finally:
            connection.close()
    except Exception as e:
        logger.error(f"ä¿å­˜æ£€æµ‹ç»“æœåˆ°æ•°æ®åº“å¤±è´¥: {e}")


def save_results_to_database(results):
    """æ‰¹é‡ä¿å­˜æ£€æµ‹ç»“æœåˆ°æ•°æ®åº“"""
    try:
        # ç¡®ä¿è¡¨å­˜åœ¨
        create_detector_result_table()
        
        # æ‰¹é‡ä¿å­˜ç»“æœ
        for result in results:
            save_result_to_database(result)
    except pymysql.MySQLError as db_err:
        # æ•°æ®åº“ç‰¹å®šé”™è¯¯å¤„ç†
        logger.error(f"æ•°æ®åº“é”™è¯¯: {db_err.args[0]}, {db_err.args[1]}")
        # æ ¹æ®é”™è¯¯ä»£ç é‡‡å–ä¸åŒçš„æ¢å¤ç­–ç•¥
        if db_err.args[0] == 1045:  # è®¿é—®è¢«æ‹’ç»
            logger.error("æ•°æ®åº“è®¤è¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç”¨æˆ·åå’Œå¯†ç ")
        elif db_err.args[0] == 1049:  # æ•°æ®åº“ä¸å­˜åœ¨
            logger.error("æ•°æ®åº“ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥é…ç½®")
        # ... å…¶ä»–æ•°æ®åº“é”™è¯¯ç±»å‹ ...
        raise
    except Exception as e:
        logger.error(f"ä¿å­˜æ£€æµ‹ç»“æœå¤±è´¥: {str(e)}")
        raise



def main():
    """ä¸»å‡½æ•° - å½©è‰²è¾“å‡ºç‰ˆ"""
    import argparse
    
    parser = argparse.ArgumentParser(description='è¿æ³•ç½‘ç«™æ‰¹é‡æ£€æµ‹å·¥å…· - å½©è‰²è¾“å‡ºç‰ˆ')
    parser.add_argument('-f', '--file', help='åŒ…å«URLåˆ—è¡¨çš„æ–‡ä»¶')
    parser.add_argument('-u', '--urls', nargs='+', help='ç›´æ¥æŒ‡å®šURLåˆ—è¡¨')
    parser.add_argument('-o', '--output', help='è¾“å‡ºæ–‡ä»¶åå‰ç¼€')
    parser.add_argument('-w', '--workers', type=int, default=10, help='å¹¶å‘å·¥ä½œçº¿ç¨‹æ•°')
    
    args = parser.parse_args()
    
    # å½©è‰²æ¬¢è¿ä¿¡æ¯
    color_printer.print_header("ğŸ›¡ï¸ è¿æ³•ç½‘ç«™æ‰¹é‡æ£€æµ‹ç³»ç»Ÿ v2.0")
    color_printer.print("ğŸ“‹ åŠŸèƒ½ç‰¹æ€§:", 'cyan', bold=True)
    color_printer.print("â€¢ å¤šç»´åº¦ç‰¹å¾åˆ†æ", 'white')
    color_printer.print("â€¢ æœºå™¨å­¦ä¹ é£é™©é¢„æµ‹", 'white') 
    color_printer.print("â€¢ å®æ—¶å½©è‰²è¾“å‡º", 'white')
    color_printer.print("â€¢ è¯¦ç»†ä¸­æ–‡æŠ¥å‘Š", 'white')
    print()
    # ä»æ•°æ®åº“æ›´æ–°æ¶æ„åŸŸååŠæ¶æ„IPæ–‡ä»¶
    update_blacklist_from_db()
    # è·å–URLåˆ—è¡¨
    urls = []
    if args.file:
        try:
            with open(args.file, 'r', encoding='utf-8') as f:
                urls = [line.strip() for line in f if line.strip()]
            color_printer.print_success(f"æˆåŠŸè¯»å– {len(urls)} ä¸ªURL")
        except Exception as e:
            color_printer.print_error(f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
            return
    elif args.urls:
        urls = args.urls
        color_printer.print_success(f"æ£€æµ‹åˆ° {len(urls)} ä¸ªURLå‚æ•°")
    else:
        # ä½¿ç”¨ç¤ºä¾‹URLè¿›è¡Œæµ‹è¯•
        urls = get_urls_from_mysql()
        # if not urls:
        #     # å¦‚æœä»æ•°æ®åº“è·å–å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨çš„ç¤ºä¾‹URL
        #     color_printer.print_warning("ä»æ•°æ®åº“è·å–URLå¤±è´¥ï¼Œä½¿ç”¨ç¤ºä¾‹URLè¿›è¡Œæµ‹è¯•")
        #     urls = [
        #         "http://36guanxiang.com",
        #         "http://51dxjy.com", 
        #         "http://etdd.cn",
        #         "http://fast024.com",
        #         "http://gsmoyy.com",
        #         "http://hry.lmtc.work"
        #     ]
        #     color_printer.print_info("ä½¿ç”¨ç¤ºä¾‹URLè¿›è¡Œæµ‹è¯•")
        # else:
        #     color_printer.print_info("ä»æ•°æ®åº“è·å–URLæˆåŠŸ")
    
    if not urls:
        color_printer.print_error("æ²¡æœ‰æä¾›å¾…æ£€æµ‹çš„URL")
        return
    
    # æ‰§è¡Œæ£€æµ‹
    detector = BatchDetector(max_workers=args.workers)
    color_printer.print(f"ğŸš€ å¼€å§‹æ£€æµ‹ {len(urls)} ä¸ªç½‘ç«™...", 'cyan', bold=True)
    
    results = detector.detect_batch(urls)
    
    # ä¿å­˜ç»“æœ
    color_printer.print_info("æ­£åœ¨ä¿å­˜æ£€æµ‹ç»“æœ...")
    json_file, csv_file = detector.save_results(args.output)
    
    # ç”Ÿæˆå¹¶ä¿å­˜æŠ¥å‘Š
    color_printer.print_info("æ­£åœ¨ç”Ÿæˆæ£€æµ‹æŠ¥å‘Š...")
    report = detector.generate_report()
    
    # ä¿å­˜æŠ¥å‘Š
    report_file = f"{args.output or 'report'}_summary.txt"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)
    
    # å½©è‰²å®Œæˆä¿¡æ¯
    color_printer.print_header("ğŸ‰ æ£€æµ‹å®Œæˆï¼")
    color_printer.print(f"ğŸ“ ç»“æœæ–‡ä»¶:", 'cyan')
    color_printer.print(f"â€¢ JSONè¯¦ç»†æ•°æ®: {json_file}", 'white')
    color_printer.print(f"â€¢ CSVç®€è¦ç»“æœ: {csv_file}", 'white')  
    color_printer.print(f"â€¢ ä¸­æ–‡æ£€æµ‹æŠ¥å‘Š: {report_file}", 'white')
    
    # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
    detector.print_summary(results)

if __name__ == '__main__':
    # è®¾ç½®ä¿¡å·å¤„ç†ï¼Œå…è®¸ä¼˜é›…é€€å‡º
    def signal_handler(sig, frame):
        print('\nğŸ›‘ ç¨‹åºå·²åœæ­¢')
        sys.exit(0)
    
    signal.signal(signal.SIGINT, signal_handler)
    
    print("ğŸš€ è¿æ³•ç½‘ç«™æ‰¹é‡æ£€æµ‹ç³»ç»Ÿå·²å¯åŠ¨")
    print("ğŸ“… å®šæ—¶ä»»åŠ¡ï¼šæ¯éš”10ç§’è¿è¡Œä¸€æ¬¡æ£€æµ‹")
    print("âŒ¨ï¸  æŒ‰ Ctrl+C å¯ä»¥éšæ—¶åœæ­¢ç¨‹åº\n")
    
    # å®šæ—¶ä»»åŠ¡ä¸»å¾ªç¯
    iteration = 1
    try:
        while True:
            print(f"\nğŸ”„ ç¬¬{iteration}è½®æ£€æµ‹å¼€å§‹")
            start_time = time.time()
            
            # æ‰§è¡Œä¸»æ£€æµ‹é€»è¾‘
            main()
            
            # è®¡ç®—æœ¬æ¬¡æ‰§è¡Œè€—æ—¶
            elapsed_time = time.time() - start_time
            print(f"âœ… ç¬¬{iteration}è½®æ£€æµ‹å®Œæˆï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’")
            
            # å¢åŠ è½®æ¬¡è®¡æ•°
            iteration += 1
            
            # ç­‰å¾…10ç§’åå†æ¬¡æ‰§è¡Œ
            wait_time = 10
            print(f"â³ ç­‰å¾…{wait_time}ç§’åè¿›è¡Œä¸‹ä¸€è½®æ£€æµ‹...")
            time.sleep(wait_time)
            
    except Exception as e:
        print(f"âŒ ç¨‹åºè¿è¡Œå‡ºé”™: {e}")
        

    
